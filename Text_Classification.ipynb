{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMqKBypMaKTpPdzQET8g6uc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EPRADDH/NLP_Natural_Language_Processing_Methods/blob/main/Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_CEI68ubkh2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9nQ3ENqeMOW"
      },
      "source": [
        "# Text classification, \r\n",
        " is defined as a technique to systematically classify a text object (document or sentence) in one of the fixed category. It is really helpful when the amount of data is too large\r\n",
        "\r\n",
        " example : there is few documents A1 A2,A3,A4 class- A,B\r\n",
        "\r\n",
        " text classification model predict\r\n",
        "\r\n",
        "  Doc   -->  class\r\n",
        "\r\n",
        " A1,A4 ==> B\r\n",
        "\r\n",
        " A2,A3 ==> A\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9V6BXrFfeW7"
      },
      "source": [
        "#Here is a code that uses naive bayes classifier using text blob library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfDFJJf5eSyb"
      },
      "source": [
        "from textblob.classifiers import NaiveBayesClassifier as NBC\r\n",
        "from textblob import TextBlob\r\n",
        "training_corpus = [\r\n",
        "                   ('I am exhausted of this work.', 'Class_B'),\r\n",
        "                   (\"I can't cooperate with this\", 'Class_B'),\r\n",
        "                   ('He is my badest enemy!', 'Class_B'),\r\n",
        "                   ('My management is poor.', 'Class_B'),\r\n",
        "                   ('I love this burger.', 'Class_A'),\r\n",
        "                   ('This is an brilliant place!', 'Class_A'),\r\n",
        "                   ('I feel very good about these dates.', 'Class_A'),\r\n",
        "                   ('This is my best work.', 'Class_A'),\r\n",
        "                   (\"What an awesome view\", 'Class_A'),\r\n",
        "                   ('I do not like this dish', 'Class_B')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwKodfWRfo6F"
      },
      "source": [
        "test_corpus = [\r\n",
        "                (\"I am not feeling well today.\", 'Class_B'), \r\n",
        "                (\"I feel brilliant!\", 'Class_A'), \r\n",
        "                ('Gary is a friend of mine.', 'Class_A'), \r\n",
        "                (\"I can't believe I'm doing this.\", 'Class_B'), \r\n",
        "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CaNi_VUf659",
        "outputId": "b44384ef-e00c-44b0-edfb-4fa500c384bc"
      },
      "source": [
        "import nltk\r\n",
        "nltk.download('punkt')\r\n",
        "  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJdvh-64ftpL",
        "outputId": "8480c594-30b8-4959-ad72-e22e7cb837a3"
      },
      "source": [
        "model = NBC(training_corpus) \r\n",
        "print(model.classify(\"Their codes are amazing.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class_A\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMLKNlBxfx5u",
        "outputId": "c89e3786-7802-489c-c4a7-ceca8e85810f"
      },
      "source": [
        "print(model.classify(\"I don't like their computer.\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class_B\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPoJej9Mf_JX",
        "outputId": "81699151-cbef-4212-8ad8-36326b2f1eef"
      },
      "source": [
        "print(model.accuracy(test_corpus))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8333333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYM3E_wogJ74"
      },
      "source": [
        "Scikit.Learn also provides a pipeline framework for text classification:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o963uAzfgILz"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.metrics import classification_report\r\n",
        "from sklearn import svm "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bA8hxUqgBa_"
      },
      "source": [
        "# preparing data for SVM model (using the same training_corpus, test_corpus from naive bayes example)\r\n",
        "train_data = []\r\n",
        "train_labels = []\r\n",
        "for row in training_corpus:\r\n",
        "    train_data.append(row[0])\r\n",
        "    train_labels.append(row[1])\r\n",
        "\r\n",
        "test_data = [] \r\n",
        "test_labels = [] \r\n",
        "for row in test_corpus:\r\n",
        "    test_data.append(row[0]) \r\n",
        "    test_labels.append(row[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0k-GqV6hE50"
      },
      "source": [
        "# Create feature vectors \r\n",
        "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\r\n",
        "# Train the feature vectors\r\n",
        "train_vectors = vectorizer.fit_transform(train_data)\r\n",
        "# Apply model on test data \r\n",
        "test_vectors = vectorizer.transform(test_data)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ut6vGdFhQPJ"
      },
      "source": [
        "# Perform classification with SVM, kernel=linear \r\n",
        "model = svm.SVC(kernel='linear') \r\n",
        "model.fit(train_vectors, train_labels) \r\n",
        "prediction = model.predict(test_vectors)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iT8PfRWHhjTc",
        "outputId": "e743bb2c-1753-43fd-8306-eb33969f0a3a"
      },
      "source": [
        "prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Class_A', 'Class_A', 'Class_B', 'Class_B', 'Class_A', 'Class_A'],\n",
              "      dtype='<U7')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_la0fyl9hbVy",
        "outputId": "98c90580-8f6c-4824-d2eb-da8b675c2ef8"
      },
      "source": [
        "print (classification_report(test_labels, prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "     Class_A       0.50      0.67      0.57         3\n",
            "     Class_B       0.50      0.33      0.40         3\n",
            "\n",
            "    accuracy                           0.50         6\n",
            "   macro avg       0.50      0.50      0.49         6\n",
            "weighted avg       0.50      0.50      0.49         6\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypkDMP8-hhar"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}