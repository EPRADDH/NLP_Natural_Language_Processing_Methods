{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis useing a LogisticRegression classifier in Python.ipynb",
      "provenance": [],
      "mount_file_id": "1BqYjuYNROdJtb5maA0zlVKM57xwV080w",
      "authorship_tag": "ABX9TyPpvGTEtscQRyhMlODoHa8Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EPRADDH/NLP_Natural_Language_Processing_Methods/blob/main/Sentiment_Analysis_useing_a_LogisticRegression_classifier_in_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVjDBW5mud8j",
        "outputId": "a9326aeb-a300-4964-9c18-0b815bdbb4c2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zawvK72tt4w3"
      },
      "source": [
        "**Loading the Data from a Data Set**\n",
        "\n",
        "Sentiment Labelled Sentences Data Set available from the UCI Machine Learning Repository."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "Oco0nEDtb-2y",
        "outputId": "4d392224-b51d-4213-eec1-819e94a65fe6"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/MyDrive/NLP-Natural-Language-Processing-Methods/sentiment labelled sentences/amazon_cells_labelled.txt', names=['review', 'sentiment'], sep='\\t')\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>So there is no way for me to plug it in here i...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Good case, Excellent value.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Great for the jawbone.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Tied to charger for conversations lasting more...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The mic is great.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review  sentiment\n",
              "0  So there is no way for me to plug it in here i...          0\n",
              "1                        Good case, Excellent value.          1\n",
              "2                             Great for the jawbone.          1\n",
              "3  Tied to charger for conversations lasting more...          0\n",
              "4                                  The mic is great.          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jd9KpAnGvtqv"
      },
      "source": [
        "**Splitting the Data Set into a Training Set and a Test Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hFj0zc7uRPc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "reviews = df['review'].values\n",
        "labels = df['sentiment'].values\n",
        "reviews_train, reviews_test, y_train, y_test = train_test_split(reviews, labels, test_size=0.2, random_state=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quf6j7YAzfno"
      },
      "source": [
        "To train an ML model and then test it, we need a way to represent the text data numerically.\n",
        "\n",
        "This can be done with the technology known as Bag of Words (BoW). You can generate a BoW matrix for text data with sklearn‘s CountVectorizer() function. This fuction is designed to convert text into numerical feature vectors, first performing tokenization and filtering of stopwords. The CountVectorizer() performs tokenization using either the default tokenizer or a custom one\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IADY9GpNz53f"
      },
      "source": [
        "**Create a Custom Tokenizer Using spaCy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucGhOIW1wE7q"
      },
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "import string\n",
        "punctuations = string.punctuation\n",
        "parser = English()\n",
        "stopwords = list(STOP_WORDS)\n",
        "def spacy_tokenizer(utterance):\n",
        "      tokens = parser(utterance)\n",
        "      return [token.lemma_.lower().strip() for token in tokens if token.text.lower().strip() not in stopwords and token.text not in punctuations]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2qkWfDy1IGA"
      },
      "source": [
        "**Transforming Text into Numerical Feature Vectors**\n",
        "\n",
        "As mentioned, transforming text to feature vectors can be done with sklearn‘s CountVectorizer() function. In the example below, we use the spaCy’s custom tokenizer created in the previous section. Alternatively, you might use the default option, passing no parameters to CountVectorizer()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3qFKpvN0vm8"
      },
      "source": [
        "#using custom function\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(tokenizer = spacy_tokenizer, ngram_range=(1,1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCjCGWPf1o_i"
      },
      "source": [
        "#By default, the vectorizer might be created as follows:\n",
        "#vectorizer = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tl1Xdmvh1xJl",
        "outputId": "014d2ef6-c618-42ea-e011-0dfc49d7a779"
      },
      "source": [
        "vectorizer.fit(reviews_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:507: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\"The parameter 'token_pattern' will not be used\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
              "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=<function spacy_tokenizer at 0x7f6c83bd1b70>,\n",
              "                vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixo1XkCp19hC"
      },
      "source": [
        "**Below transform the text into numerical feature vectors:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_-mvriA1yYP"
      },
      "source": [
        "X_train = vectorizer.transform(reviews_train)\n",
        "X_test = vectorizer.transform(reviews_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyRdAFh935Os"
      },
      "source": [
        "**Training the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnj1Nhyo2Hq5",
        "outputId": "82426339-42fc-48af-d79b-64d1ddea0727"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgNH1xxk4JLC"
      },
      "source": [
        " **Evaluation of the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j98lh0DB3lfq",
        "outputId": "3d8fc667-0108-482f-b32b-9575293709a2"
      },
      "source": [
        "accuracy = classifier.score(X_test, y_test)\n",
        "print(\"Accuracy: %s\" % (accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TAb4lJG5uiT"
      },
      "source": [
        "This means that the accuracy of our model is 76%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JeWsfUNB5yI_"
      },
      "source": [
        "**Predictions on New Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GB0ph9JR4Sy2",
        "outputId": "fc62fb45-f067-4462-f110-1bba32c1f957"
      },
      "source": [
        "new_reviews = ['Old version of python useless', 'Very good effort, but not five stars', 'Clear and concise']\n",
        "X_new = vectorizer.transform(new_reviews)\n",
        "classifier.predict(X_new)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18Wn7_E16oO5"
      },
      "source": [
        "As you can see, the model have worked fine for the above reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7_awHge6CCP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}