{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Extracting Keywords with TF-IDF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNbLtiqGExJe6xaBoj+cU00",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EPRADDH/NLP_Natural_Language_Processing_Methods/blob/main/Extracting_Keywords_with_TF_IDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7V1NIaEC4jV"
      },
      "source": [
        "# You may be wondering why you would even use TF-IDF for any task at all ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4phdz_SIDOtf"
      },
      "source": [
        "The truth is TF-IDF is easy to understand, easy to compute and is one of the most versatile statistic that shows the relative importance of a word or phrase in a document or a set of documents in comparison to the rest of your corpus.\n",
        "\n",
        "TF-IDF can be used for a wide range of tasks including text classification, clustering / topic-modeling, search, keyword extraction and a whole lot more.\n",
        "\n",
        "In this article, we will see how to use TF-IDF from the scikit-learn package to extract keywords from documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VU65PRAhLDlK"
      },
      "source": [
        "[TF-IDF  videos online](https://www.youtube.com/results?search_query=tf-idf.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_n22VzKLq4l"
      },
      "source": [
        "weâ€™ll use a stack overflow dataset for Keyword extraction.\n",
        "We will be using two files\n",
        "\n",
        "1.stackoverflow-data-idf.json has 20,000 posts\n",
        "\n",
        "2.stackoverflow-test.json has 500 posts \n",
        "\n",
        "where first one used to compute the Inverse Document Frequency (IDF) and  another one, we would use  as a test set for us to extract keywords from.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3wzc_ukWPId",
        "outputId": "526953dc-3e0c-4816-d6f9-ab9de565b54a"
      },
      "source": [
        "#Drive Mount\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftKoQ9eja_lT"
      },
      "source": [
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jKAfRxs6CyHI"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# read json into a dataframe Here, lines=True simply means we are treating each line in the text file as a separate json string.\n",
        "\n",
        "df_idf=pd.read_json(\"/content/drive/MyDrive/NLP-Natural-Language-Processing-Methods/keyword extraction using TFIDF/stackoverflow-data-idf.json\",lines=True,encoding='cp1252')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVxViPC3_KNe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "CJ-pX-YmTntJ",
        "outputId": "06495c05-c369-4db7-b588-607ffb26e5e7"
      },
      "source": [
        "df_idf.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>body</th>\n",
              "      <th>answer_count</th>\n",
              "      <th>comment_count</th>\n",
              "      <th>creation_date</th>\n",
              "      <th>last_activity_date</th>\n",
              "      <th>last_editor_display_name</th>\n",
              "      <th>owner_display_name</th>\n",
              "      <th>owner_user_id</th>\n",
              "      <th>post_type_id</th>\n",
              "      <th>score</th>\n",
              "      <th>tags</th>\n",
              "      <th>view_count</th>\n",
              "      <th>accepted_answer_id</th>\n",
              "      <th>favorite_count</th>\n",
              "      <th>last_edit_date</th>\n",
              "      <th>last_editor_user_id</th>\n",
              "      <th>community_owned_date</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4821394</td>\n",
              "      <td>Serializing a private struct - Can it be done?</td>\n",
              "      <td>&lt;p&gt;I have a public class that contains a priva...</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2011-01-27 20:19:13.563 UTC</td>\n",
              "      <td>2011-01-27 20:21:37.59 UTC</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>163534.0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>c#|serialization|xml-serialization</td>\n",
              "      <td>296</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3367882</td>\n",
              "      <td>How do I prevent floated-right content from ov...</td>\n",
              "      <td>&lt;p&gt;I have the following HTML:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;cod...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2010-07-30 00:01:50.9 UTC</td>\n",
              "      <td>2012-05-10 14:16:05.143 UTC</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "      <td>1190.0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>css|overflow|css-float|crop</td>\n",
              "      <td>4121</td>\n",
              "      <td>3367943.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2012-05-10 14:16:05.143 UTC</td>\n",
              "      <td>44390.0</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        id  ... community_owned_date\n",
              "0  4821394  ...                  NaN\n",
              "1  3367882  ...                  NaN\n",
              "\n",
              "[2 rows x 19 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSjnl-DJTtoK",
        "outputId": "0a4226c3-501d-4b21-cc24-13b4c0b75dff"
      },
      "source": [
        "# print schema\n",
        "print(\"Schema:\\n\\n\",df_idf.dtypes)\n",
        "print(\"Number of questions,columns=\",df_idf.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Schema:\n",
            "\n",
            " id                            int64\n",
            "title                        object\n",
            "body                         object\n",
            "answer_count                  int64\n",
            "comment_count                 int64\n",
            "creation_date                object\n",
            "last_activity_date           object\n",
            "last_editor_display_name     object\n",
            "owner_display_name           object\n",
            "owner_user_id               float64\n",
            "post_type_id                  int64\n",
            "score                         int64\n",
            "tags                         object\n",
            "view_count                    int64\n",
            "accepted_answer_id          float64\n",
            "favorite_count              float64\n",
            "last_edit_date               object\n",
            "last_editor_user_id         float64\n",
            "community_owned_date         object\n",
            "dtype: object\n",
            "Number of questions,columns= (20000, 19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PPm4gEVb4Eh"
      },
      "source": [
        "this dataset contains 19 fields.but we only intrusted in the body and title which will become our source of text for keyword extraction.\n",
        "\n",
        "We will now create a field that combines both body and title so we have it in one field. We will also print the second text entry in our new field just to see what the text looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kknotQ8SYSp5"
      },
      "source": [
        "import re\n",
        "def pre_process(text):\n",
        "    \n",
        "    # lowercase\n",
        "    text=text.lower()\n",
        "\n",
        "    #remove tags\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "    \n",
        "    \n",
        "    #remove tags\n",
        "    text=re.sub(\"\",\"\",text)\n",
        "    \n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    \n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BM8tDP_rchwl"
      },
      "source": [
        "df_idf['text'] = df_idf['title'] + df_idf['body']\n",
        "df_idf['text'] = df_idf['text'].apply(lambda x:pre_process(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "vOV-xDLwcph2",
        "outputId": "1c5b69b6-1c23-41c9-d938-3e609f7ad253"
      },
      "source": [
        "#show the 'text' to check how it is\n",
        "df_idf['text'][3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'loop variable as parameter in asynchronous function call p i have an object with the following form p pre code sortedfilters task appointment email code pre p now i want to use a code for in code loop to build my tasks array for code async parallel code which contains the functions to be executed asynchronously p pre code var tasks for var entity in sortedfilters tasks push function callback var entityresult fetchrecordsforentity entity businessunits sortedfilters var formattedentityresults formatresults entity entityresult callback null formattedentityresults code pre p the problem is that at the moment the functions are called code entity code points at the last value of the loop in this case it would be code email code p p how can i nail the exact value at the moment the function is added to the array p '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4E3t9dNdwGi"
      },
      "source": [
        "All of the cleaning happens in pre_process(..). You can do a lot more stuff in pre_process(..), such as eliminate all code sections, normalize the words to its root, etc, but for simplicity we perform only some mild pre-processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcdZbJ3Pd9rT"
      },
      "source": [
        "# Creating Vocabulary and Word Counts for IDF\n",
        "\n",
        "Now we need to create the vocabulary and start the counting process. We can use the CountVectorizer to create a vocabulary from all the text in our df_idf['text']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sfoVPqocvYC"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import re\n",
        "\n",
        "def get_stop_words(stop_file_path):\n",
        "    \"\"\"load stop words \"\"\"\n",
        "    \n",
        "    with open(stop_file_path, 'r', encoding=\"utf-8\") as f:\n",
        "        stopwords = f.readlines()\n",
        "        stop_set = set(m.strip() for m in stopwords)\n",
        "        return frozenset(stop_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xd37PQ1Vf9WG",
        "outputId": "437f717f-9410-4fc9-9886-22fc0877ddd7"
      },
      "source": [
        "#load a set of stop words\n",
        "#stopwords=get_stop_words(\"resources/stopwords.txt\")\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSSMNZZGigEU"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "sw = stopwords.words(\"english\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "9p4d_lTMjbtp",
        "outputId": "55ce8a49-5051-4a95-c90c-9ea01bef0f9a"
      },
      "source": [
        "#get the text column \n",
        "docs=df_idf['text'].tolist()\n",
        "\n",
        "docs[3]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'loop variable as parameter in asynchronous function call p i have an object with the following form p pre code sortedfilters task appointment email code pre p now i want to use a code for in code loop to build my tasks array for code async parallel code which contains the functions to be executed asynchronously p pre code var tasks for var entity in sortedfilters tasks push function callback var entityresult fetchrecordsforentity entity businessunits sortedfilters var formattedentityresults formatresults entity entityresult callback null formattedentityresults code pre p the problem is that at the moment the functions are called code entity code points at the last value of the loop in this case it would be code email code p p how can i nail the exact value at the moment the function is added to the array p '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFZhwLlX1yrR"
      },
      "source": [
        "# create a vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSBqw4yfj2qd"
      },
      "source": [
        "#create a vocabulary of words, \n",
        "#ignore words that appear in 85% of documents, \n",
        "#eliminate stop words\n",
        "cv=CountVectorizer(max_df=0.85,stop_words=sw)\n",
        "word_count_vector=cv.fit_transform(docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biPmCcSnkkvP"
      },
      "source": [
        "While cv.fit(...) would only create the vocabulary, cv.fit_transform(...) creates the vocabulary and returns a term-document matrix which is what we want.in this, each column in the matrix represents a word in the vocabulary while each row represents the document in our dataset where the values in this case are the word counts. Note that with this representation, counts of some words could be 0 if the word did not appear in the corresponding document"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l7RNwURkPHW",
        "outputId": "cdaadbde-c426-4585-9ac6-8e42024818ae"
      },
      "source": [
        "word_count_vector.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 121098)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqVFgvlqlzJ6"
      },
      "source": [
        "The resulting shape of word_count_vector is (20000,127626) since we have 20,000 documents in our dataset (the rows) and the vocabulary size is 127626. In some text mining applications such as clustering and text classification we typically limit the size of the vocabulary. Itâ€™s really easy to do this by setting max_features=vocab_size when instantiating CountVectorizer. For this tutorial letâ€™s limit our vocabulary size to 10,000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z17UpJN_kYz-"
      },
      "source": [
        "cv=CountVectorizer(max_df=0.85,stop_words=sw,max_features=10000)\n",
        "word_count_vector=cv.fit_transform(docs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPo1u0AFmFfh",
        "outputId": "6b9c9935-5b36-4451-e0f2-2b1e3c99f84e"
      },
      "source": [
        "word_count_vector.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20000, 10000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1SR-krEm1_s"
      },
      "source": [
        "Now, letâ€™s look at 50 words from our vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BhnD1kZmvPz",
        "outputId": "81bfe81b-ef37-4054-82e2-9fdb6eed4af7"
      },
      "source": [
        "list(cv.vocabulary_.keys())[:50]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['serializing',\n",
              " 'private',\n",
              " 'struct',\n",
              " 'done',\n",
              " 'public',\n",
              " 'class',\n",
              " 'contains',\n",
              " 'properties',\n",
              " 'mostly',\n",
              " 'string',\n",
              " 'want',\n",
              " 'serialize',\n",
              " 'attempt',\n",
              " 'stream',\n",
              " 'disk',\n",
              " 'using',\n",
              " 'xmlserializer',\n",
              " 'get',\n",
              " 'error',\n",
              " 'saying',\n",
              " 'types',\n",
              " 'serialized',\n",
              " 'need',\n",
              " 'way',\n",
              " 'keep',\n",
              " 'prevent',\n",
              " 'floated',\n",
              " 'right',\n",
              " 'content',\n",
              " 'overlapping',\n",
              " 'main',\n",
              " 'following',\n",
              " 'html',\n",
              " 'pre',\n",
              " 'code',\n",
              " 'lt',\n",
              " 'gt',\n",
              " 'long',\n",
              " 'fit',\n",
              " 'space',\n",
              " 'cut',\n",
              " 'display',\n",
              " 'follows',\n",
              " 'icon',\n",
              " 'css',\n",
              " 'td',\n",
              " 'span',\n",
              " 'overflow',\n",
              " 'hidden',\n",
              " 'white']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed4vCtBaxFK1"
      },
      "source": [
        "# TfidfTransformer to Compute Inverse Document Frequency (IDF)\n",
        "Itâ€™s now time to compute the IDF values. In the code below, we are essentially taking the sparse matrix from CountVectorizer (word_count_vector) to generate the IDF when you invoke tfidf_transformer.fit(...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u3FI9A6m9Py",
        "outputId": "167e1292-43f9-4dde-cd5b-bcd3bb6c0931"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(word_count_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "Q6026ZCTxT0Q",
        "outputId": "0c506b19-b6e4-4bc3-9d08-2cdd92f0bc3f"
      },
      "source": [
        "# print idf values \n",
        "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
        " \n",
        "# sort ascending \n",
        "df_idf.sort_values(by=['idf_weights'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>idf_weights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>code</th>\n",
              "      <td>1.242695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>pre</th>\n",
              "      <td>1.373653</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>using</th>\n",
              "      <td>2.015471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gt</th>\n",
              "      <td>2.028950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lt</th>\n",
              "      <td>2.117845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>itargetspeed</th>\n",
              "      <td>10.210390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_name</th>\n",
              "      <td>10.210390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>item_level</th>\n",
              "      <td>10.210390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>itick</th>\n",
              "      <td>10.210390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>zzzzzzzzzzzzzzzzzzzzzzzzz</th>\n",
              "      <td>10.210390</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                           idf_weights\n",
              "code                          1.242695\n",
              "pre                           1.373653\n",
              "using                         2.015471\n",
              "gt                            2.028950\n",
              "lt                            2.117845\n",
              "...                                ...\n",
              "itargetspeed                 10.210390\n",
              "row_name                     10.210390\n",
              "item_level                   10.210390\n",
              "itick                        10.210390\n",
              "zzzzzzzzzzzzzzzzzzzzzzzzz    10.210390\n",
              "\n",
              "[10000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KEtK6V9BzpTl"
      },
      "source": [
        "# Computing TF-IDF and Extracting Keywords\n",
        "The important point to note here is that the IDF should always be based on a large corpora and should be representative of texts you would be using to extract keywords.\n",
        "\n",
        "Once we have our IDF computed, we are now ready to compute TF-IDF and then extract top keywords from the TF-IDF vectors.\n",
        "\n",
        "here we will extract top keywords for the questions in data/stackoverflow-test.json. This data file has 500 questions with fields identical to that of data/stackoverflow-data-idf.json as we saw above. We will start by reading our test file, extracting the necessary fields (title and body) and getting the texts into a list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrRQJSVMyxL3"
      },
      "source": [
        "# read test docs into a dataframe and concatenate title and body\n",
        "df_test=pd.read_json(\"/content/drive/MyDrive/NLP-Natural-Language-Processing-Methods/keyword extraction using TFIDF/stackoverflow-test.json\",lines=True,encoding='cp1252')\n",
        "df_test['text'] = df_test['title'] + df_test['body']\n",
        "df_test['text'] =df_test['text'].apply(lambda x:pre_process(x))\n",
        "\n",
        "# get test docs into a list\n",
        "docs_test=df_test['text'].tolist()\n",
        "docs_title=df_test['title'].tolist()\n",
        "docs_body=df_test['body'].tolist()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD8LW0Eh0_Ll",
        "outputId": "1b186ccb-e398-4f5f-83e2-f68c2423b922"
      },
      "source": [
        "docs_test[:4]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['integrate war plugin for m eclipse into eclipse project p i set up a small web project with jsf and maven now i want to deploy on a tomcat server is there a possibility to automate that like a button in eclipse that automatically deploys the project to tomcat p p i read about a the a href http maven apache org plugins maven war plugin rel nofollow noreferrer maven war plugin a but i couldn t find a tutorial how to integrate that into my process eclipse m eclipse p p can you link me to help or try to explain it thanks p ',\n",
              " 'phantomjs node page evaulate seems to hang p i have an implementation of waitfor with phantomjs node and it seems that the code sitepage evaluate code has a big lag compared to when it should evaluate true you ll see below that i m logging out the content value and the content logs with what should evaluate as true but this doesn t seem to occur for a good seconds or so after the fact p p any idea what would cause this delay or if there s a better way to evaluate p pre code let promise require bluebird let phantom require phantom let sitepage let phinstance phantom create then instance gt phinstance instance return instance createpage then page gt sitepage page return page open https thepiratebay org search game then status gt return waituntil function this returns the correct content after a short period while the evaluate ends up taking maybe s longer after this content should evaluate true sitepage property content then content gt console log content return sitepage evaluate function return document getelementbyid searchresult then function return sitepage property content catch promise timeouterror function e sitepage close phinstance exit then content gt console log content console log content sitepage close phinstance exit catch error gt console log error phinstance exit var waituntil asynctest gt return new promise function resolve reject function wait console log waiting asynctest then function value if value console log resolve resolve else settimeout wait catch function e console log error found rejecting e reject wait code pre ',\n",
              " 'dynamic operations can only be performed in homogenous appdomain p i m working with an api that requires p pre code lt gt code pre p to be set in my web config file in order to work this works without a problem in vs but when i use vs i get an error stating that dynamic operations can only be performed in homogeneous appdomain the project is targeting net framework and is an asp net mvc project p p i tried changing legacycasmodel to false but then i can t access the object i need not sure how to resolve this issue how can i use legacycasmodel true in vs with dynamic expressions p ',\n",
              " 'css with relative url to background image p i have a file structure of p pre code home html img bg_damask jpg css style css code pre p when i set my body background image i can t get it to load i ve tried p pre code background image url img bg_damask jpg background image url img bg_damask jpg background image url img bg_damask jpg code pre p but none are working how do i get my css to reference the background image p p eta in browser dev tools i see that no matter what file path i put in the browser is only referencing bg_damask jpg without the file path if i edit it in dev tools the image shows up using option now i m stumped as to what s causing the breakdown p ']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdyg-enj1ZWk"
      },
      "source": [
        "The next step is to compute the tf-idf value for a given document in our test set by invoking tfidf_transformer.transform(...). This generates a vector of tf-idf scores. Next, we sort the words in the vector in descending order of tf-idf values and then iterate over to extract the top-n keywords. In the example below, we are extracting keywords for the first document in our test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auY6RczM65Gp"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6mDoLcJ65dS"
      },
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "    \n",
        "    # word index and corresponding tf-idf score\n",
        "    for idx, score in sorted_items:\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkqNhmA568Ic"
      },
      "source": [
        "The sort_coo(...) method essentially sorts the values in the vector while preserving the column index. Once you have the column index then its really easy to look-up the corresponding word value as you would see in extract_topn_from_vector(...)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j2PZz75d1HG0"
      },
      "source": [
        "# we only needs to do this once, this is a mapping of index to \n",
        "feature_names=cv.get_feature_names()\n",
        "\n",
        "# get the document that we want to extract keywords from\n",
        "doc=docs_test[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfa5yWmA2Npd"
      },
      "source": [
        "#generate tf-idf for the given document\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwEzwLDQ2Op8"
      },
      "source": [
        "#sort the tf-idf vectors by descending order of scores\n",
        "\n",
        "sorted_items=sort_coo(tf_idf_vector.tocoo())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75_K7zki2QoG"
      },
      "source": [
        "#extract only the top n; n here is 10\n",
        "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1smJbMyi74pO",
        "outputId": "91ed195c-3951-48ef-c7ed-0802812f204d"
      },
      "source": [
        "# now print the results\n",
        "print(\"\\n=====Doc=====\")\n",
        "print(doc)\n",
        "print(\"\\n===Keywords===\")\n",
        "for k in keywords:\n",
        "    print(k,keywords[k])\n",
        "\n",
        "\n",
        "# now print the results\n",
        "print(\"\\n=====Title=====\")\n",
        "print(docs_title[0])\n",
        "print(\"\\n=====Body=====\")\n",
        "print(docs_body[0])\n",
        "print(\"\\n===Keywords===\")\n",
        "for k in keywords:\n",
        "    print(k,keywords[k])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=====Doc=====\n",
            "integrate war plugin for m eclipse into eclipse project p i set up a small web project with jsf and maven now i want to deploy on a tomcat server is there a possibility to automate that like a button in eclipse that automatically deploys the project to tomcat p p i read about a the a href http maven apache org plugins maven war plugin rel nofollow noreferrer maven war plugin a but i couldn t find a tutorial how to integrate that into my process eclipse m eclipse p p can you link me to help or try to explain it thanks p \n",
            "\n",
            "===Keywords===\n",
            "eclipse 0.492\n",
            "maven 0.453\n",
            "war 0.395\n",
            "plugin 0.267\n",
            "integrate 0.234\n",
            "tomcat 0.224\n",
            "project 0.198\n",
            "automate 0.131\n",
            "jsf 0.126\n",
            "possibility 0.122\n",
            "\n",
            "=====Title=====\n",
            "Integrate War-Plugin for m2eclipse into Eclipse Project\n",
            "\n",
            "=====Body=====\n",
            "<p>I set up a small web project with JSF and Maven. Now I want to deploy on a Tomcat server. Is there a possibility to automate that like a button in Eclipse that automatically deploys the project to Tomcat?</p>\n",
            "\n",
            "<p>I read about a the <a href=\"http://maven.apache.org/plugins/maven-war-plugin/\" rel=\"nofollow noreferrer\">Maven War Plugin</a> but I couldn't find a tutorial how to integrate that into my process (eclipse/m2eclipse).</p>\n",
            "\n",
            "<p>Can you link me to help or try to explain it. Thanks.</p>\n",
            "\n",
            "===Keywords===\n",
            "eclipse 0.492\n",
            "maven 0.453\n",
            "war 0.395\n",
            "plugin 0.267\n",
            "integrate 0.234\n",
            "tomcat 0.224\n",
            "project 0.198\n",
            "automate 0.131\n",
            "jsf 0.126\n",
            "possibility 0.122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCHD-M5d78fY"
      },
      "source": [
        "\n",
        "# put the common code into several methods\n",
        "def get_keywords(idx):\n",
        "\n",
        "    #generate tf-idf for the given document\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs_test[idx]]))\n",
        "\n",
        "    #sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,20)\n",
        "    \n",
        "    return keywords\n",
        "\n",
        "def print_results(idx,keywords):\n",
        "    # now print the results\n",
        "    print(\"\\n=====Title=====\")\n",
        "    print(docs_title[idx])\n",
        "    print(\"\\n=====Body=====\")\n",
        "    print(docs_body[idx])\n",
        "    print(\"\\n===Keywords===\")\n",
        "    for k in keywords:\n",
        "        print(k,keywords[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOT4LIQu8xEP",
        "outputId": "dfe1c274-d02f-4f07-8606-062274c23f51"
      },
      "source": [
        "#Now let's look at keywords generated for a much longer question:\n",
        "\n",
        "idx=120\n",
        "keywords=get_keywords(idx)\n",
        "print_results(idx,keywords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=====Title=====\n",
            "SQL Import Wizard - Error\n",
            "\n",
            "=====Body=====\n",
            "<p>I have a CSV file that I'm trying to import into SQL Management Server Studio.</p>\n",
            "\n",
            "<p>In Excel, the column giving me trouble looks like this:\n",
            "<a href=\"https://i.stack.imgur.com/pm0uS.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/pm0uS.png\" alt=\"enter image description here\"></a></p>\n",
            "\n",
            "<p>Tasks > import data > Flat Source File > select file</p>\n",
            "\n",
            "<p><a href=\"https://i.stack.imgur.com/G4b6I.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/G4b6I.png\" alt=\"enter image description here\"></a></p>\n",
            "\n",
            "<p>I set the data type for this column to DT_NUMERIC, adjust the DataScale to 2 in order to get 2 decimal places, but when I click over to Preview, I see that it's clearly not recognizing the numbers appropriately:</p>\n",
            "\n",
            "<p><a href=\"https://i.stack.imgur.com/NZhiQ.png\" rel=\"nofollow noreferrer\"><img src=\"https://i.stack.imgur.com/NZhiQ.png\" alt=\"enter image description here\"></a></p>\n",
            "\n",
            "<p>The column mapping for this column is set to type = decimal; precision 18; scale 2.</p>\n",
            "\n",
            "<p>Error message: Data Flow Task 1: Data conversion failed. The data conversion for column \"Amount\" returned status value 2 and status text \"The value could not be converted because of a potential loss of data.\".\n",
            " (SQL Server Import and Export Wizard)</p>\n",
            "\n",
            "<p>Can someone identify where I'm going wrong here?  Thanks!</p>\n",
            "\n",
            "===Keywords===\n",
            "imgur 0.283\n",
            "png 0.28\n",
            "stack 0.27\n",
            "column 0.253\n",
            "https 0.216\n",
            "import 0.197\n",
            "data 0.195\n",
            "wizard 0.186\n",
            "com 0.184\n",
            "pm 0.162\n",
            "decimal 0.156\n",
            "conversion 0.153\n",
            "sql 0.149\n",
            "description 0.145\n",
            "alt 0.145\n",
            "enter 0.144\n",
            "img 0.139\n",
            "src 0.135\n",
            "noreferrer 0.133\n",
            "image 0.126\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1j1Lmjv84BY",
        "outputId": "412413c5-1184-4129-c264-39af9e1ef8f4"
      },
      "source": [
        "idx=122\n",
        "keywords=get_keywords(idx)\n",
        "print_results(idx,keywords)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=====Title=====\n",
            "Retrieving server address from email\n",
            "\n",
            "=====Body=====\n",
            "<p>How can I retrieve mail's SMTP server from email (for example oneat@op.pl) ?</p>\n",
            "\n",
            "===Keywords===\n",
            "email 0.452\n",
            "smtp 0.361\n",
            "op 0.352\n",
            "pl 0.34\n",
            "retrieving 0.326\n",
            "server 0.322\n",
            "mail 0.286\n",
            "retrieve 0.251\n",
            "address 0.225\n",
            "example 0.146\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AF62mi6tHx13"
      },
      "source": [
        "#Generate keywords for a batch of documents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "TfeAu-ItGPCv",
        "outputId": "8b021273-2333-4aef-c43c-8cf6b36cc12f"
      },
      "source": [
        "#generate tf-idf for all documents in your list. docs_test has 500 documents\n",
        "tf_idf_vector=tfidf_transformer.transform(cv.transform(docs_test))\n",
        "\n",
        "results=[]\n",
        "for i in range(tf_idf_vector.shape[0]):\n",
        "    \n",
        "    # get vector for a single document\n",
        "    curr_vector=tf_idf_vector[i]\n",
        "    \n",
        "    #sort the tf-idf vector by descending order of scores\n",
        "    sorted_items=sort_coo(curr_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "    \n",
        "    \n",
        "    results.append(keywords)\n",
        "\n",
        "df=pd.DataFrame(zip(docs,results),columns=['doc','keywords'])\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>doc</th>\n",
              "      <th>keywords</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>serializing a private struct can it be done p ...</td>\n",
              "      <td>{'eclipse': 0.492, 'maven': 0.453, 'war': 0.39...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>how do i prevent floated right content from ov...</td>\n",
              "      <td>{'evaluate': 0.464, 'content': 0.408, 'console...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gradle command line p i m trying to run a shel...</td>\n",
              "      <td>{'appdomain': 0.397, 'dynamic': 0.37, 'vs': 0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>loop variable as parameter in asynchronous fun...</td>\n",
              "      <td>{'background': 0.43, 'image': 0.397, 'jpg': 0....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>canot get the href value p hi i need to valid ...</td>\n",
              "      <td>{'uri': 0.369, 'bitmap': 0.315, 'intent': 0.30...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>how to unbind click and click submit button in...</td>\n",
              "      <td>{'delphi': 0.685, 'compatible': 0.31, 'win': 0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>496</th>\n",
              "      <td>swaggerui auth redirect swaggeruiauth of null ...</td>\n",
              "      <td>{'node': 0.515, 'null': 0.328, 'selectsingleno...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>497</th>\n",
              "      <td>ssrs value display error for ssrs conditional ...</td>\n",
              "      <td>{'logo': 0.539, 'triangle': 0.313, 'step': 0.3...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>498</th>\n",
              "      <td>accessing and changing a class instance from a...</td>\n",
              "      <td>{'code': 0.452, 'length': 0.344, 'ev': 0.334, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>499</th>\n",
              "      <td>how to print the current time in the format da...</td>\n",
              "      <td>{'localhost': 0.453, 'oauth': 0.365, 'sdk': 0....</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>500 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                   doc                                           keywords\n",
              "0    serializing a private struct can it be done p ...  {'eclipse': 0.492, 'maven': 0.453, 'war': 0.39...\n",
              "1    how do i prevent floated right content from ov...  {'evaluate': 0.464, 'content': 0.408, 'console...\n",
              "2    gradle command line p i m trying to run a shel...  {'appdomain': 0.397, 'dynamic': 0.37, 'vs': 0....\n",
              "3    loop variable as parameter in asynchronous fun...  {'background': 0.43, 'image': 0.397, 'jpg': 0....\n",
              "4    canot get the href value p hi i need to valid ...  {'uri': 0.369, 'bitmap': 0.315, 'intent': 0.30...\n",
              "..                                                 ...                                                ...\n",
              "495  how to unbind click and click submit button in...  {'delphi': 0.685, 'compatible': 0.31, 'win': 0...\n",
              "496  swaggerui auth redirect swaggeruiauth of null ...  {'node': 0.515, 'null': 0.328, 'selectsingleno...\n",
              "497  ssrs value display error for ssrs conditional ...  {'logo': 0.539, 'triangle': 0.313, 'step': 0.3...\n",
              "498  accessing and changing a class instance from a...  {'code': 0.452, 'length': 0.344, 'ev': 0.334, ...\n",
              "499  how to print the current time in the format da...  {'localhost': 0.453, 'oauth': 0.365, 'sdk': 0....\n",
              "\n",
              "[500 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re-eB6qWH1Ng"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YwKz5IY2Kl9D"
      },
      "source": [
        "#What is Keyword Extraction?\n",
        "Keyword extraction is defined as the task of Natural language processing that automatically identifies a set of terms to describe the subject of the text. This is an important method in information retrieval (IR) systems: keywords simplify and speed up research. Keyword extraction can be used to reduce text dimensionality for further text analysis (subject modeling text classification)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsZvZU7-K9P1"
      },
      "source": [
        "lets take one more task on Keyword Extraction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "Sw9d8keBK5xe",
        "outputId": "cd314ecd-3288-4a4a-ac26-402430463dba"
      },
      "source": [
        "import pandas as pd # data processing\n",
        "df = pd.read_csv('C:/Users/Pradeep Dhote/Documents/NLP/NLP_Natural_Language_Processing_Methods/keyword_extraction_using_tfidf/papers.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-111-bdc16821dd3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m \u001b[0;31m# data processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'C:/Users/Pradeep Dhote/Documents/NLP/NLP_Natural_Language_Processing_Methods/keyword_extraction_using_tfidf/papers.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/Pradeep Dhote/Documents/NLP/NLP_Natural_Language_Processing_Methods/keyword_extraction_using_tfidf/papers.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Arq3fXMK2Ojn"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waS_Wlk6P78_"
      },
      "source": [
        "Dataset To upload from your local drive,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "zSD59pWvO7y3",
        "outputId": "1b24002f-9619-4d29-a524-881f9397e728"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "import io\n",
        "df2 = pd.read_csv(io.BytesIO(uploaded['papers.csv']))\n",
        "# Dataset is now stored in a Pandas Dataframe"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-307b3d7b-0feb-4bf3-9bfa-b5d6aca1bb48\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-307b3d7b-0feb-4bf3-9bfa-b5d6aca1bb48\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving papers.csv to papers (1).csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-23bf401cfae2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muploaded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'papers.csv'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     result = _output.eval_js(\n\u001b[1;32m     71\u001b[0m         'google.colab._files._uploadFilesContinue(\"{output_id}\")'.format(\n\u001b[0;32m---> 72\u001b[0;31m             output_id=output_id))\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'append'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m       \u001b[0;31m# JS side uses a generator of promises to process all of the files- some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWugGXdH290w"
      },
      "source": [
        "import pandas as pd # data processing\n",
        "df2 = pd.read_csv('/content/papers.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP7XGt8y2t97",
        "outputId": "48bdba07-bcc4-44a7-d5d9-18501b9401a2"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzo6-mrrPp0c"
      },
      "source": [
        " import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "##Creating a list of custom stopwords\n",
        "new_words = [\"fig\",\"figure\",\"image\",\"sample\",\"using\", \n",
        "             \"show\", \"result\", \"large\", \n",
        "             \"also\", \"one\", \"two\", \"three\", \n",
        "             \"four\", \"five\", \"seven\",\"eight\",\"nine\"]\n",
        "stop_words = list(stop_words.union(new_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ydQRoPYtiRs"
      },
      "source": [
        "def pre_process(text):\n",
        "    \n",
        "    # lowercase\n",
        "    text=text.lower()\n",
        "    \n",
        "    #remove tags\n",
        "    text=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",text)\n",
        "    \n",
        "    # remove special characters and digits\n",
        "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
        "    \n",
        "    ##Convert to list from string\n",
        "    text = text.split()\n",
        "    \n",
        "    # remove stopwords\n",
        "    text = [word for word in text if word not in stop_words]\n",
        "\n",
        "    # remove words less than three letters\n",
        "    text = [word for word in text if len(word) >= 3]\n",
        "\n",
        "    # lemmatize\n",
        "    lmtzr = WordNetLemmatizer()\n",
        "    text = [lmtzr.lemmatize(word) for word in text]\n",
        "    \n",
        "    return ' '.join(text)\n",
        "docs = df2['paper_text'].apply(lambda x:pre_process(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cu00VTKmwFP9"
      },
      "source": [
        "#Using TF-IDF\n",
        "\n",
        "Using the tf-idf weighting scheme, the keywords are the words with the highest TF-IDF score. For this task, Iâ€™ll first use the CountVectorizer method in Scikit-learn to create a vocabulary and generate the word count:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zHbuL5rt0E-"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "#docs = docs.tolist()\n",
        "#create a vocabulary of words, \n",
        "cv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n",
        "                   max_features=10000,  # the size of the vocabulary\n",
        "                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n",
        "                  )\n",
        "word_count_vector=cv.fit_transform(docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5vzgZtEy5QL",
        "outputId": "f4cb7179-6f5f-4c25-8959-8eebe13bdf90"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "\n",
        "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
        "tfidf_transformer.fit(word_count_vector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-f8qmqizxNZ"
      },
      "source": [
        "Now, we are ready for the final step. In this step, I will create a function for the task of Keyword Extraction with Python by using the Tf-IDF vectorization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pVcGZ1l0AOR"
      },
      "source": [
        "def sort_coo(coo_matrix):\n",
        "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
        "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
        "\n",
        "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
        "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
        "    \n",
        "    #use only topn items from vector\n",
        "    sorted_items = sorted_items[:topn]\n",
        "\n",
        "    score_vals = []\n",
        "    feature_vals = []\n",
        "\n",
        "    for idx, score in sorted_items:\n",
        "        fname = feature_names[idx]\n",
        "        \n",
        "        #keep track of feature name and its corresponding score\n",
        "        score_vals.append(round(score, 3))\n",
        "        feature_vals.append(feature_names[idx])\n",
        "\n",
        "    #create a tuples of feature,score\n",
        "    #results = zip(feature_vals,score_vals)\n",
        "    results= {}\n",
        "    for idx in range(len(feature_vals)):\n",
        "        results[feature_vals[idx]]=score_vals[idx]\n",
        "    \n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu_khD7k0MAq"
      },
      "source": [
        "# get feature names\n",
        "feature_names=cv.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vR8SX9Im0S2Y"
      },
      "source": [
        "def get_keywords(idx, docs):\n",
        "\n",
        "    #generate tf-idf for the given document\n",
        "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs[idx]]))\n",
        "\n",
        "    #sort the tf-idf vectors by descending order of scores\n",
        "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
        "\n",
        "    #extract only the top n; n here is 10\n",
        "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
        "    \n",
        "    return keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jybEo9cT0T8-"
      },
      "source": [
        "def print_results(idx,keywords, df2):\n",
        "    # now print the results\n",
        "    print(\"\\n=====Title=====\")\n",
        "    print(df2['title'][idx])\n",
        "    print(\"\\n=====Abstract=====\")\n",
        "    print(df2['abstract'][idx])\n",
        "    print(\"\\n===Keywords===\")\n",
        "    for k in keywords:\n",
        "        print(k,keywords[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCNWg2Lg0doB",
        "outputId": "8dcb858e-8921-4199-f8d3-c39f3eb6bd87"
      },
      "source": [
        "idx=999\n",
        "keywords=get_keywords(idx, docs)\n",
        "print_results(idx,keywords, df2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "=====Title=====\n",
            "Shape Context: A New Descriptor for Shape Matching and Object Recognition\n",
            "\n",
            "=====Abstract=====\n",
            "Abstract Missing\n",
            "\n",
            "===Keywords===\n",
            "shape 0.686\n",
            "matching 0.249\n",
            "prototype 0.24\n",
            "descriptor 0.201\n",
            "point 0.188\n",
            "object 0.167\n",
            "context 0.157\n",
            "distance 0.119\n",
            "correspondence 0.111\n",
            "recognition 0.104\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9q4EYGDW7Lyh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}