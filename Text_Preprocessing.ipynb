{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOD7Vt/y/uD6GSnKy2lBDHm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EPRADDH/NLP_Natural_Language_Processing_Methods/blob/main/Text_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot4PQ67OoWBa"
      },
      "source": [
        "NLP and its components, one can organize the massive chunks of text data, perform numerous automated tasks and solve a wide range of problems such as ‚Äì automatic summarization, machine translation, named entity recognition, relationship extraction, sentiment analysis, speech recognition, and topic segmentation etc.\n",
        "\n",
        "# [Text Preprocessing](https://medium.com/@annabiancajones/sentiment-analysis-of-reviews-text-pre-processing-6359343784fb)\n",
        "![the architecture of text preprocessing pipeline](https://cdn.analyticsvidhya.com/wp-content/uploads/2017/01/11180616/Image-1.png)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gCSX6112a11"
      },
      "source": [
        "[Efficient Text Data Cleaning methods](https://www.geeksforgeeks.org/python-efficient-text-data-cleaning/) with example "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtMK5WaQhKvY"
      },
      "source": [
        "#Noise removal \n",
        "\n",
        "Method :1\n",
        "\n",
        "is to prepare a dictionary of noisy entities, and iterate the text object by tokens (or by words), eliminating those tokens which are present in the noise dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Q4CD_kKXawG8",
        "outputId": "20d15017-d563-4c11-9ff0-79be53960c16"
      },
      "source": [
        "# Sample code to remove noisy words from a text\n",
        "\n",
        "noise_list = [\"is\", \"a\", \"this\", \"...\"] \n",
        "def _remove_noise(input_text):\n",
        "    words = input_text.split() \n",
        "    noise_free_words = [word for word in words if word not in noise_list] \n",
        "    noise_free_text = \" \".join(noise_free_words) \n",
        "    return noise_free_text\n",
        "\n",
        "_remove_noise(\"this is a sample text\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'sample text'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKd9iFiHha0P"
      },
      "source": [
        "Method :2\n",
        "\n",
        "# Tokenization : is the task of split a text into a meaningful segment called tokan.\n",
        "the input of the tokenizer is unicode and output is Doc object"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfAmddW0kVbG"
      },
      "source": [
        "example_text = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.This is just one minor example, but imagine every word in the English language, every possible tense and affix you can put on a word. Having individual dictionary entries per version would be highly redundant and inefficient, especially since, once we convert to numbers, the  is going to be identical.\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xOd6FlwkqDq",
        "outputId": "84216cf8-126d-409f-cab5-8ba099b09338"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GEaknNwHhaEU",
        "outputId": "d27e2bcc-b7ef-4cf2-f399-35ba8c842d10"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "# break paragraph to sentance \n",
        "sent_tokenize(example_text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello Mr. Smith, how are you doing today?',\n",
              " 'The weather is great, and Python is awesome.',\n",
              " 'The sky is pinkish-blue.',\n",
              " \"You shouldn't eat cardboard.This is just one minor example, but imagine every word in the English language, every possible tense and affix you can put on a word.\",\n",
              " 'Having individual dictionary entries per version would be highly redundant and inefficient, especially since, once we convert to numbers, the  is going to be identical.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4x4RewRUkc0o",
        "outputId": "d81e6d00-c439-446b-cd07-6bfe7b6c18aa"
      },
      "source": [
        "# break sentance to word\n",
        "words = word_tokenize(example_text)\n",
        "words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Mr.',\n",
              " 'Smith',\n",
              " ',',\n",
              " 'how',\n",
              " 'are',\n",
              " 'you',\n",
              " 'doing',\n",
              " 'today',\n",
              " '?',\n",
              " 'The',\n",
              " 'weather',\n",
              " 'is',\n",
              " 'great',\n",
              " ',',\n",
              " 'and',\n",
              " 'Python',\n",
              " 'is',\n",
              " 'awesome',\n",
              " '.',\n",
              " 'The',\n",
              " 'sky',\n",
              " 'is',\n",
              " 'pinkish-blue',\n",
              " '.',\n",
              " 'You',\n",
              " 'should',\n",
              " \"n't\",\n",
              " 'eat',\n",
              " 'cardboard.This',\n",
              " 'is',\n",
              " 'just',\n",
              " 'one',\n",
              " 'minor',\n",
              " 'example',\n",
              " ',',\n",
              " 'but',\n",
              " 'imagine',\n",
              " 'every',\n",
              " 'word',\n",
              " 'in',\n",
              " 'the',\n",
              " 'English',\n",
              " 'language',\n",
              " ',',\n",
              " 'every',\n",
              " 'possible',\n",
              " 'tense',\n",
              " 'and',\n",
              " 'affix',\n",
              " 'you',\n",
              " 'can',\n",
              " 'put',\n",
              " 'on',\n",
              " 'a',\n",
              " 'word',\n",
              " '.',\n",
              " 'Having',\n",
              " 'individual',\n",
              " 'dictionary',\n",
              " 'entries',\n",
              " 'per',\n",
              " 'version',\n",
              " 'would',\n",
              " 'be',\n",
              " 'highly',\n",
              " 'redundant',\n",
              " 'and',\n",
              " 'inefficient',\n",
              " ',',\n",
              " 'especially',\n",
              " 'since',\n",
              " ',',\n",
              " 'once',\n",
              " 'we',\n",
              " 'convert',\n",
              " 'to',\n",
              " 'numbers',\n",
              " ',',\n",
              " 'the',\n",
              " 'is',\n",
              " 'going',\n",
              " 'to',\n",
              " 'be',\n",
              " 'identical',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8t0Zo1a-lZ_f"
      },
      "source": [
        "# Stopword :# removing supoting words which is not meaningful or inportant\n",
        "\n",
        "Example : is ,the ,are, etc\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsXqC7tMlwxC",
        "outputId": "166e12be-3360-4a51-bfad-045c1dc13bf5"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U60gLw0YbmaO"
      },
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4zU6TbRbiya"
      },
      "source": [
        "import NLP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLFr40MrtI4H"
      },
      "source": [
        "."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzXpmPYLtLmj",
        "outputId": "6a986897-2a1e-49f4-80ed-397700ff5de6"
      },
      "source": [
        "# Print the set of spaCy's default stop words (remember that sets are unordered):\n",
        "print(nlp.Defaults.stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'front', 'he', 'however', 'cannot', 'only', 'per', 'being', 'empty', 'quite', 'towards', 'done', 'though', 'part', 'sometime', 'very', 'serious', \"'ve\", 'across', 'below', 'been', 'somewhere', 'others', 'not', 'whenever', '‚Äôs', 'please', 'something', 'regarding', 'hereafter', 'off', 'yourselves', 'put', 'now', 'himself', 'any', 'no', 'thereafter', 'then', 'too', 'hundred', 'her', 'you', 'in', 'nothing', 'ever', 'could', 'whether', 'hence', 'be', 'may', 'three', 'do', 'have', 'just', 'therein', 'those', 'twelve', 'several', 'it', 'up', 'whence', 'to', 'yours', 'their', 'amongst', 'indeed', 'least', 'herself', 'upon', 'already', 'although', '‚Äòll', 'wherever', 'before', 'sometimes', 'full', 'get', '‚Äôll', 'and', 'has', 'top', 'used', 'your', '‚Äòve', 'besides', 'but', 'can', 'did', 'eight', 'more', 'other', 'out', 'most', 'due', 'into', 'thereupon', 'yet', 'always', 'of', 'among', 'everyone', 'should', 'take', 'became', 'former', 'or', 'formerly', 'own', 'two', 'anything', 'much', 'noone', 'name', 'toward', 'after', 'ca', 'move', 'once', 'many', 'between', 'each', 'from', 'forty', 'a', 'less', 'sixty', 'such', 'this', 'therefore', 'his', 'we', 'behind', 'otherwise', \"'d\", 'am', 'anyone', 'back', 'first', 'him', 'whom', '‚Äòre', 'last', 'doing', 'hers', 'here', 'next', 'since', 'throughout', 'without', 'by', 'down', 'is', 'latter', 'she', 'elsewhere', 'n‚Äòt', 'nine', 'meanwhile', 'which', 'were', 'who', \"'ll\", 'whole', 'as', 'every', \"n't\", 'are', 'because', 'ourselves', 'above', 'see', 'they', 'nowhere', 'seems', 'thus', 'that', 'whereby', 'with', 'will', 'whatever', 'one', 'us', 'nor', 'latterly', 'through', 'why', 'within', 'none', 'our', 'under', 'where', 'yourself', 'never', 'herein', 'against', 'over', 'along', 'become', 'my', 'there', 'afterwards', 'go', 'keep', 'another', 'seem', 'anywhere', 'made', 'neither', 'whose', 'at', 'all', 'for', 'whereas', 'whereupon', 'these', 'onto', 'about', 'also', 'enough', 'six', 'few', 'seeming', 'either', 'give', 'the', 'themselves', 'unless', 'was', 'via', '‚Äòm', 'becomes', 'show', 'really', 'using', 'bottom', 'would', 'myself', 'again', 'nevertheless', 'fifteen', 'does', 'further', 'thereby', '‚Äôm', 'even', 'an', 'on', 'while', '‚Äôre', 'four', 'whoever', 'me', 'what', '‚Äôd', 'mine', 'so', 'except', 'well', \"'m\", 'if', 'might', 'various', 'together', 'mostly', 'how', 'thence', \"'s\", 'its', 'around', 'amount', 'itself', 'twenty', 'anyhow', 'call', 'beforehand', \"'re\", 'whither', 'perhaps', 'than', 'five', 'eleven', 'still', 'ours', 'beyond', 'becoming', 're', 'during', 'hereby', 'often', 'ten', 'somehow', 'them', 'must', 'almost', 'i', 'had', 'moreover', 'same', 'some', 'someone', 'everything', 'until', 'hereupon', 'rather', 'third', 'everywhere', 'beside', '‚Äòd', 'say', 'make', 'thru', 'whereafter', 'wherein', '‚Äòs', '‚Äôve', 'nobody', 'when', 'else', 'n‚Äôt', 'both', 'alone', 'seemed', 'anyway', 'side', 'fifty', 'namely'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MvK8P4COlP54",
        "outputId": "f8334a04-1d22-43d3-a58c-f2e7ed949ccb"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_word = set(stopwords.words('english'))\n",
        "filter_sent = []\n",
        "for w in words:\n",
        "    if w not in stop_word:\n",
        "        filter_sent.append(w)\n",
        "print(filter_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'luv', '<3', 'iphone', '&', 'youre', 'awsm', 'apple.DisplayIsAwesome,', 'sooo', 'happppppy', 'http://www.apple.com']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUGCqFvBlni3",
        "outputId": "297bb2c0-cc37-4766-8f2a-1ce5dd725bed"
      },
      "source": [
        "# using list comprehension\n",
        "filter_sent = [w for w in words if w not in stop_word]\n",
        "print(filter_sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['I', 'luv', '<3', 'iphone', '&', 'youre', 'awsm', 'apple.DisplayIsAwesome,', 'sooo', 'happppppy', 'http://www.apple.com']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uumSbZRtS-R",
        "outputId": "236c249c-b1e2-4c82-8c47-9a063b22a921"
      },
      "source": [
        "len(nlp.Defaults.stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNnlGEIHtYb4",
        "outputId": "c6256c8f-2979-48fb-9b93-70b20197d6e2"
      },
      "source": [
        "nlp.vocab['myself'].is_stop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qf2Kk9bJtfsx"
      },
      "source": [
        "# To add a stop word\n",
        "There may be times when you wish to add a stop word to the default set. Perhaps you decide that 'btw' (common shorthand for \"by the way\") should be considered a stop word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xYfEUNbStlxH"
      },
      "source": [
        "# Add the word to the set of stop words. Use lowercase!\n",
        "nlp.Defaults.stop_words.add('btw')\n",
        "\n",
        "# Set the stop_word tag on the lexeme\n",
        "nlp.vocab['btw'].is_stop = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BacHCkGntnbz",
        "outputId": "64ea82c4-d6a5-4e26-dc37-446c0932577d"
      },
      "source": [
        "len(nlp.Defaults.stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "327"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dnCKGcqtySP",
        "outputId": "41ea0f7e-9fd2-4050-ea08-f90741e3abfb"
      },
      "source": [
        "nlp.vocab['btw'].is_stop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW1jculKtzpV"
      },
      "source": [
        "#To remove a stop word\n",
        "Alternatively, you may decide that 'beyond' should not be considered a stop word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZkC9zIxt37j"
      },
      "source": [
        "# Remove the word from the set of stop words\n",
        "nlp.Defaults.stop_words.remove('beyond')\n",
        "\n",
        "# Remove the stop_word tag from the lexeme\n",
        "nlp.vocab['beyond'].is_stop = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMZyLSvPt4Af",
        "outputId": "e80e0e3b-95b5-4cf3-83a4-6c66f6f6269f"
      },
      "source": [
        "len(nlp.Defaults.stop_words)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "326"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhFyB0e0t_PB",
        "outputId": "d2993b73-7d57-42c4-9d76-218700e74f4e"
      },
      "source": [
        "nlp.vocab['beyond'].is_stop"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQUCSb9jpyu0"
      },
      "source": [
        "Sometimes, we may wish to break a sentence into a list of words.\n",
        "\n",
        "In such cases, we may first want to clean up the string and remove all the punctuation marks. Here is an example of how it is done"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6NyryAshqGJ2",
        "outputId": "1cb5aac2-6636-4c69-c309-c7808a70d5c8"
      },
      "source": [
        "# import string library function  \n",
        "import string  \n",
        "    \n",
        "# Storing the sets of punctuation in variable result  \n",
        "result = string.punctuation  \n",
        "    \n",
        "# Printing the punctuation values  \n",
        "print(result)  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-QZRWI4oJzZ",
        "outputId": "c7c43d7e-660e-4035-cdb1-6b45844b740b"
      },
      "source": [
        "# define punctuation\n",
        "punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "\n",
        "my_str = \"Hello!!!, he said ---and went.\"\n",
        "\n",
        "# To take input from the user\n",
        "# my_str = input(\"Enter a string: \")\n",
        "\n",
        "# remove punctuation from the string\n",
        "no_punct = \"\"\n",
        "for char in my_str:\n",
        "   if char not in punctuations:\n",
        "       no_punct = no_punct + char\n",
        "\n",
        "# display the unpunctuated string\n",
        "print(no_punct)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello he said and went\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sng4yeUBp3Eh",
        "outputId": "58eb0f7f-3bff-4acb-da1b-ec9367249594"
      },
      "source": [
        "# remove punctuation from the string result\n",
        "no_punct = \"\"\n",
        "for char in my_str:\n",
        "   if char not in result:\n",
        "       no_punct = no_punct + char\n",
        "\n",
        "# display the unpunctuated string\n",
        "print(no_punct)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello he said and went\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjoA6xRErFVz"
      },
      "source": [
        "#Stemming : stemming is a sort of normalizing method.\n",
        "Stemming is the process of producing morphological variants of a root/base word. \n",
        "\n",
        "Stemming programs are commonly referred to as stemming algorithms or stemmers.\n",
        "\n",
        "Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems\n",
        "\n",
        " A stemming algorithm reduces the wordsto the root word\n",
        " \n",
        " example ward like\n",
        "  ‚Äúchocolates‚Äù, ‚Äúchocolatey‚Äù, ‚Äúchoco‚Äù to the root word, ‚Äúchocolate‚Äù and ‚Äúretrieval‚Äù, ‚Äúretrieved‚Äù, ‚Äúretrieves‚Äù reduce to the  ‚Äúretrieve‚Äù"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32XBELvgrFpM"
      },
      "source": [
        "Applications of stemming are:\n",
        "\n",
        "Stemming is used in information retrieval systems like search engines.\n",
        "\n",
        "It is used to determine domain vocabularies in domain analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZoaHiQhqSVv",
        "outputId": "19566e15-dcc8-4bac-b356-8fb69b5c3e86"
      },
      "source": [
        "# import these modules \n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk.tokenize import word_tokenize \n",
        "   \n",
        "ps = PorterStemmer() \n",
        "  \n",
        "# choose some words to be stemmed \n",
        "words = [\"program\", \"programs\", \"programer\", \"programing\", \"programers\", \"chocolates\", \"chocolatey\", \"choco\" ] \n",
        "  \n",
        "for w in words: \n",
        "    print(w, \" : \", ps.stem(w)) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "program  :  program\n",
            "programs  :  program\n",
            "programer  :  program\n",
            "programing  :  program\n",
            "programers  :  program\n",
            "chocolates  :  chocol\n",
            "chocolatey  :  chocolatey\n",
            "choco  :  choco\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p20_N8iQrGtQ",
        "outputId": "2d4020ed-b328-4fb8-9742-3680187f7579"
      },
      "source": [
        "from nltk.stem import LancasterStemmer, SnowballStemmer\n",
        "lancaster = LancasterStemmer()\n",
        "print(\"LancasterStemmer\")\n",
        "for w in words: \n",
        "    print(w, \" : \", lancaster.stem(w))\n",
        "\n",
        "porter = PorterStemmer()\n",
        "print(\"PorterStemmer\")\n",
        "for w in words: \n",
        "    print(w, \" : \", porter.stem(w)) \n",
        "  \n",
        "Snowball = SnowballStemmer(\"english\")\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LancasterStemmer\n",
            "program  :  program\n",
            "programs  :  program\n",
            "programer  :  program\n",
            "programing  :  program\n",
            "programers  :  program\n",
            "chocolates  :  chocol\n",
            "chocolatey  :  chocolatey\n",
            "choco  :  choco\n",
            "PorterStemmer\n",
            "program  :  program\n",
            "programs  :  program\n",
            "programer  :  program\n",
            "programing  :  program\n",
            "programers  :  program\n",
            "chocolates  :  chocol\n",
            "chocolatey  :  chocolatey\n",
            "choco  :  choco\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE5uZjavvoOp"
      },
      "source": [
        "# Lemmatization\n",
        "\n",
        "Lemmatization is the process of grouping together the different forms of a word so they can be analysed as a single item. \n",
        "\n",
        "Lemmatization is similar to stemming but it brings context to the words. So it links words with similar meaning to one word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGXWjDrs3cNE"
      },
      "source": [
        "#Diffrance between Stemming and Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmC8hW6WvogE"
      },
      "source": [
        "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TioViIr3Mqk"
      },
      "source": [
        "stemming technique only looks at the form of the word whereas lemmatization technique looks at the meaning of the word. It means after applying lemmatization, we will always get a valid word."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHjmo0tD43bu",
        "outputId": "2ae0995b-2ca5-4289-ba02-59c8867a3836"
      },
      "source": [
        "nltk.download('wordnet')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_G1epp4jux15",
        "outputId": "992d81ca-1dd1-4a58-c851-e877c493a431"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize('eating')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'eating'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M8o6dCB04zkE",
        "outputId": "0be9759b-1b42-419e-b1cc-c91a2852a2e9"
      },
      "source": [
        "for w in words: \n",
        "    print(w, \" : \", lemmatizer.lemmatize(w))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "program  :  program\n",
            "programs  :  program\n",
            "programer  :  programer\n",
            "programing  :  programing\n",
            "programers  :  programers\n",
            "chocolates  :  chocolate\n",
            "chocolatey  :  chocolatey\n",
            "choco  :  choco\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PYo5RFv5yDG"
      },
      "source": [
        "# WordNet\n",
        "is a lexical database for the English language.to find the meanings of words, synonyms, antonyms, and more. Let's cover some examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r94eFZxX5HRb",
        "outputId": "680dfdf8-a198-49c1-f740-ed67355206cb"
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "synonyms = []\n",
        "antonyms = []\n",
        "\n",
        "for syn in wordnet.synsets(\"friend\"):\n",
        "    for l in syn.lemmas():\n",
        "        synonyms.append(l.name())\n",
        "        if l.antonyms():\n",
        "            antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "print(set(synonyms))\n",
        "print(set(antonyms))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'supporter', 'ally', 'protagonist', 'champion', 'admirer', 'booster', 'Quaker', 'friend', 'Friend', 'acquaintance'}\n",
            "{'stranger', 'foe'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cZxXNCj571H",
        "outputId": "02eccbb8-30f5-4048-ca77-3411b501ae8b"
      },
      "source": [
        "synonyms = []\n",
        "antonyms = []\n",
        "\n",
        "for syn in wordnet.synsets(\"beautiful\"):\n",
        "    for l in syn.lemmas():\n",
        "        synonyms.append(l.name())\n",
        "        if l.antonyms():\n",
        "            antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "print(set(synonyms))\n",
        "print(set(antonyms))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'beautiful'}\n",
            "{'ugly'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0SCeTpQ6eYG"
      },
      "source": [
        "# Note\n",
        "Next, we can also easily use WordNet to compare the similarity of two words and their tenses, by incorporating the Wu and Palmer method for semantic related-ness.\n",
        "Let's compare the noun of \"ship\" and \"boat:\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZDo-U7tH6SUZ",
        "outputId": "26b2b118-7d51-4e3b-ad03-9dd7bfac4cd7"
      },
      "source": [
        "w1 = wordnet.synset('ship.n.01')\n",
        "w2 = wordnet.synset('boat.n.01')\n",
        "print(w1.wup_similarity(w2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9090909090909091\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CPF0mJN6mUb",
        "outputId": "f2076339-7b9f-443e-c676-b1e4d9be80d7"
      },
      "source": [
        "w1 = wordnet.synset('ship.n.01')\n",
        "w2 = wordnet.synset('car.n.01')\n",
        "print(w1.wup_similarity(w2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.6956521739130435\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aT43aRO6oOe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zurEAAvb68aA"
      },
      "source": [
        "# Bag of Words (BOW) \n",
        "is a method to extract features from text documents.  It creates a vocabulary of all the unique words occurring in all the documents in the training set. It converts the documents to a fixed-length vector of numbers.create vector table or sparce matric."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5nmIw76JvNZ"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_jNPXbaaqea"
      },
      "source": [
        "# Object Standardization\n",
        "\n",
        "Text data often contains words or phrases which are not present in any standard lexical dictionaries. These pieces are not recognized by search engines and models.\n",
        "\n",
        "Some of the examples are ‚Äì acronyms, hashtags with attached words, and colloquial slangs. With the help of regular expressions and manually prepared data dictionaries, this type of noise can be fixed, the code below uses a dictionary lookup method to replace social media slangs from a text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "fJIgMrJqixOP",
        "outputId": "390783b5-fadf-428f-9df5-53b1208f9298"
      },
      "source": [
        "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\", }\n",
        "def _lookup_words(input_text):\n",
        "    words = input_text.split() \n",
        "    new_words = [] \n",
        "    for word in words:\n",
        "        if word.lower() in lookup_dict:\n",
        "            word = lookup_dict[word.lower()]\n",
        "        new_words.append(word) \n",
        "    new_text = \" \".join(new_words)\n",
        "        \n",
        "    return new_text\n",
        "\n",
        "_lookup_words(\"RT this is a retweeted tweet by Shivam Bansal\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Retweet this is a retweeted tweet by Shivam Bansal'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_akWDWnNoDCg"
      },
      "source": [
        "[Steps for data cleaning:](https://www.analyticsvidhya.com/blog/2014/11/text-data-cleaning-steps-python/)\n",
        "\n",
        "#Escaping HTML characters :\n",
        "Data obtained from web usually contains a lot of html entities like &lt; &gt; &amp; which gets embedded in the original data. It is thus necessary to get rid of these entities. One approach is to directly remove them by the use of specific regular expressions. Another approach is to use appropriate packages and modules (for example htmlparser of Python), which can convert these entities to standard html tags. For example: &lt; is converted to ‚Äú<‚Äù and &amp; is converted to ‚Äú&"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZZ58UZ1opfI"
      },
      "source": [
        "#Decoding data: \n",
        "Thisis the process of transforming information from complex symbols to simple and easier to understand characters. Text data may be subject to different forms of decoding like ‚ÄúLatin‚Äù, ‚ÄúUTF8‚Äù etc. Therefore, for better analysis, it is necessary to keep the complete data in standard encoding format. UTF-8 encoding is widely accepted and is recommended to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MlZGWKui0Qi"
      },
      "source": [
        "import html.parser\n",
        "\n",
        "original_tweet = \"I luv my &lt;3 iphone &amp; you‚Äôre awsm apple.DisplayIsAwesome, sooo happppppy üôÇ  http://www.apple.com\"\n",
        "\n",
        "tweet = original_tweet.encode(\"utf-8\").decode(\"ascii\", errors=\"ignore\")\n",
        "\n",
        "tweet = html.unescape(tweet)\n",
        "\n",
        "#tweet = tweet.replace(\"&lt;\", \"<\").replace(\"&amp;\", \"&\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "UZOV3Otxp0kf",
        "outputId": "fe227f91-a6f8-40b1-8c2d-0825e1c12db9"
      },
      "source": [
        "tweet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I luv my <3 iphone & youre awsm apple.DisplayIsAwesome, sooo happppppy   http://www.apple.com'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4v_JoFBHq4hj"
      },
      "source": [
        "# Apostrophe Lookup:\n",
        "To avoid any word sense clerification in text, it is recommended to maintain proper structure in it and to abide by the rules of context free grammar. When apostrophes are used, chances of clerification increases.\n",
        "\n",
        "For example ‚Äúit‚Äôs is a contraction for it is or it has‚Äù.\n",
        "\n",
        "All the apostrophes should be converted into standard lexicons. One can use a lookup table of all possible keys to get rid of disambiguates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Nisz6GhAqITf",
        "outputId": "781ef590-34d1-471b-b4f2-8093d1f7dabe"
      },
      "source": [
        "APPOSTOPHES = {'s' : \" is\", 're' :\" are\",'youre':\"you are\"} \n",
        "## Need a huge dictionary\n",
        "\n",
        "words = tweet.split()\n",
        "reformed = [APPOSTOPHES[word] if word in APPOSTOPHES else word for word in words]\n",
        "\n",
        "reformed = \" \".join(reformed) \n",
        "reformed\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'I luv my <3 iphone & you are awsm apple.DisplayIsAwesome, sooo happppppy http://www.apple.com'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jy1ohyjxxSYX"
      },
      "source": [
        "#Removal of Stop-words:\n",
        " When data analysis needs to be data driven at the word level, the commonly occurring words (stop-words) should be removed. One can either create a long list of stop-words or one can use predefined language specific libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqnd3MYZxbhx"
      },
      "source": [
        "#Removal of Punctuations: \n",
        "All the punctuation marks according to the priorities should be dealt with. For example: ‚Äú.‚Äù, ‚Äú,‚Äù,‚Äù?‚Äù are important punctuations that should be retained while others need to be removed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhErvtLByfZP"
      },
      "source": [
        "#Removal of Expressions: \n",
        "Textual data (usually speech transcripts) may contain human expressions like [laughing], [Crying], [Audience paused]. These expressions are usually non relevant to content of the speech and hence need to be removed. Simple regular expression can be useful in this case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6TxMtp1yfnx"
      },
      "source": [
        "[Using Regex for Text Manipulation in Python](https://stackabuse.com/using-regex-for-text-manipulation-in-python/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eYDjE5d0_Nw"
      },
      "source": [
        "#Slangs lookup:\n",
        " Again, social media comprises of a majority of slang words. These words should be transformed into standard words to make free text. The words like luv will be converted to love, Helo to Hello. The similar approach of apostrophe look up can be used to convert slangs to standard words. A number of sources are available on the web, which provides lists of all possible slangs, this would be your holy grail and you could use them as lookup dictionaries for conversion purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G033XEPcu0aG",
        "outputId": "38fee773-aaf5-446e-a883-9f933a58e35e"
      },
      "source": [
        "#open the fle slang.txt \n",
        "#file=open(\"slang.txt\",\"r\") \n",
        "#slang=file.read() \n",
        "\n",
        "slang = 'awsm=awesome\\nasap=as soon as possible\\nb4=before\\nlol=laugh out loud'\n",
        "\n",
        "#seperating each line present in the file \n",
        "slang=slang.split('\\n') \n",
        "  \n",
        "tweet_tokens=tweet.split() \n",
        "slang_word=[] \n",
        "meaning=[] \n",
        "  \n",
        "#store the slang words and meanings in different lists \n",
        "for line in slang: \n",
        "    temp=line.split(\"=\") \n",
        "    slang_word.append(temp[0]) \n",
        "    meaning.append(temp[-1]) \n",
        "  \n",
        "#replace the slang word with meaning \n",
        "for i,word in enumerate(tweet_tokens): \n",
        "    if word in slang_word: \n",
        "        idx=slang_word.index(word) \n",
        "        tweet_tokens[i]=meaning[idx] \n",
        "          \n",
        "tweet=\" \".join(tweet_tokens) \n",
        "print(\"After slang replacement the tweet is:-\\n{}\".format(tweet)) \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After slang replacement the tweet is:-\n",
            "I luv my <3 iphone & youre awesome apple.DisplayIsAwesome, sooo happppppy http://www.apple.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PafXe72S1gbZ"
      },
      "source": [
        "#Split attached words:\n",
        "\n",
        "Some words are joined together for example ‚Äì ‚ÄúDisplayIsAwesome‚Äù. These need to be separated to be able to extract the meaning out of it. After splitting, it will be ‚ÄúDisplay Is Awesome‚Äù. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0woR9YeAvNCp",
        "outputId": "55f4fef3-d7b8-4451-8153-8b9896a6e61a"
      },
      "source": [
        "import re \n",
        "#separate the words \n",
        "tweet = \" \".join([s for s in re.split(\"([A-Z][a-z]+[^A-Z]*)\",tweet) if s]) \n",
        "print(\"After spliting attached words the tweet is:-\\n{}\".format(tweet))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After spliting attached words the tweet is:-\n",
            "I luv my <3 iphone & youre awesome apple. Display Is Awesome, sooo happppppy http://www.apple.com\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hjGqqyA66LZ"
      },
      "source": [
        "#Standardizing and Spell Check:\n",
        "There might be spelling errors in the text or it might not be in the correct format.For example: ‚ÄúI looooveee you‚Äù should be ‚ÄúI love you‚Äù.\n",
        "\n",
        " ‚Äì ‚Äúdrivng‚Äù for ‚Äúdriving‚Äù or ‚ÄúI misssss this‚Äù for ‚ÄúI miss this‚Äù. We can correct these by using the \"autocorrect library for python. There are other libraries available which you can use as well. First, you will have to install the library by using the command- "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "id": "nO9fEC2Fvbq2",
        "outputId": "2e1bd519-dfa6-407f-f5d2-4fce1d315107"
      },
      "source": [
        "#install autocorrect library\n",
        "pip install autocorrect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-66-81be8fb1bd58>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    pip install autocorrect\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uc40c7bU8HJF"
      },
      "source": [
        "tweet1 = \"I enjoyd the event which took place yesteday &amp; I luvd it ! The link to the show is http://t.co/4ftYom0i  It's awsome you'll luv it #HadFun #Enjoyed BFN GN\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "jLB3tNfC72As",
        "outputId": "98bf90d2-edb8-43c6-ad2d-80f0b9370c47"
      },
      "source": [
        "import itertools \n",
        "#One letter in a word should not be present more than twice in continuation \n",
        "tweet1 = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet1)) \n",
        "print(\"After standardizing the tweet is:-\\n{}\".format(tweet1)) \n",
        "  \n",
        "from autocorrect import Speller  \n",
        "spell = Speller(lang='en') \n",
        "#spell check \n",
        "tweet=spell(tweet1) \n",
        "print(\"After Spell check the tweet is:-\\n{}\".format(tweet1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After standardizing the tweet is:-\n",
            "I enjoyd the event which took place yesteday &amp; I luvd it ! The link to the show is http://t.co/4ftYom0i  It's awsome you'll luv it #HadFun #Enjoyed BFN GN\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-68-1a37ccf42cc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"After standardizing the tweet is:-\\n{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautocorrect\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSpeller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mspell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpeller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#spell check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'autocorrect'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx5SuG1gRveC"
      },
      "source": [
        "# Text to Features (Feature Engineering on text data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qgi0UhQJjr-w"
      },
      "source": [
        "# Sentences token\n",
        "Certain tokens inside a Doc object may also receive a \"start of sentence\" tag. While this doesn't immediately build a list of sentences, these tags enable the generation of sentence segments through Doc.sents. Later we'll write our own segmentation rules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuxplwdOj1t4"
      },
      "source": [
        "doc4 = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTxTYAe8j69l",
        "outputId": "f4fc918c-0c5d-40a4-c173-fd601900328e"
      },
      "source": [
        "for sent in doc4.sents:\n",
        "    print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is the first sentence.\n",
            "This is another sentence.\n",
            "This is the last sentence.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D87fvpRJj7Nf",
        "outputId": "47a29b34-07b5-4d1a-ecb9-5aa83a2a3c36"
      },
      "source": [
        "doc4[6].is_sent_start"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NHbtkhYjwUU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG5M4ZcnWspR"
      },
      "source": [
        "# Dependency parsing\n",
        "\n",
        "is the task of analyzing the grammatical structure of a sentence and establishing the relationships between \"head\" words and the words in a sentance\n",
        "\n",
        "dependency parsing can tell you what the subjects and objects of a verb are, as well as which words are modifying (describing) the subject.\n",
        "\n",
        " This can help you find precise answers to specific questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_iIMFrO8rIf",
        "outputId": "2de33b0c-34a0-4dc8-db95-4ba44c1abdb1"
      },
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "\n",
        "text='It took me more than two hours to translate a few pages of English.'\n",
        "\n",
        "for token in nlp(text):\n",
        " print(token.text,'=>',token.dep_,'=>',token.head.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It => nsubj => took\n",
            "took => ROOT => took\n",
            "me => dative => took\n",
            "more => amod => two\n",
            "than => quantmod => two\n",
            "two => nummod => hours\n",
            "hours => dobj => took\n",
            "to => aux => translate\n",
            "translate => xcomp => took\n",
            "a => det => pages\n",
            "few => amod => pages\n",
            "pages => dobj => translate\n",
            "of => prep => pages\n",
            "English => pobj => of\n",
            ". => punct => took\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "P2e7JYnGXm5I",
        "outputId": "0310c309-7106-4b7d-ab48-adc20432a6aa"
      },
      "source": [
        "from spacy import displacy\n",
        "displacy.render(nlp(text),style='dep', jupyter=True, options={'distance': 80})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"b1ceb3b2366048c2b98058202b927315-0\" class=\"displacy\" width=\"1170\" height=\"337.0\" direction=\"ltr\" style=\"max-width: none; height: 337.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">It</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"130\">took</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"130\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"210\">me</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"210\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">more</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"370\">than</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"370\">SCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">two</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">hours</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"690\">translate</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"690\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">few</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">pages</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">of</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1090\">English.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1090\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1ceb3b2366048c2b98058202b927315-0-0\" stroke-width=\"2px\" d=\"M70,202.0 C70,162.0 110.0,162.0 110.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1ceb3b2366048c2b98058202b927315-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,204.0 L62,192.0 78,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1ceb3b2366048c2b98058202b927315-0-1\" stroke-width=\"2px\" d=\"M150,202.0 C150,162.0 190.0,162.0 190.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1ceb3b2366048c2b98058202b927315-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dative</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M190.0,204.0 L198.0,192.0 182.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1ceb3b2366048c2b98058202b927315-0-2\" stroke-width=\"2px\" d=\"M310,202.0 C310,122.0 435.0,122.0 435.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1ceb3b2366048c2b98058202b927315-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M310,204.0 L302,192.0 318,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1ceb3b2366048c2b98058202b927315-0-3\" stroke-width=\"2px\" d=\"M390,202.0 C390,162.0 430.0,162.0 430.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1ceb3b2366048c2b98058202b927315-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M390,204.0 L382,192.0 398,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1ceb3b2366048c2b98058202b927315-0-4\" stroke-width=\"2px\" d=\"M470,202.0 C470,162.0 510.0,162.0 510.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1ceb3b2366048c2b98058202b927315-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M470,204.0 L462,192.0 478,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1ceb3b2366048c2b98058202b927315-0-5\" stroke-width=\"2px\" d=\"M150,202.0 C150,42.0 525.0,42.0 525.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1ceb3b2366048c2b98058202b927315-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M525.0,204.0 L533.0,192.0 517.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1ceb3b2366048c2b98058202b927315-0-6\" stroke-width=\"2px\" d=\"M630,202.0 C630,162.0 670.0,162.0 670.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1ceb3b2366048c2b98058202b927315-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M630,204.0 L622,192.0 638,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1ceb3b2366048c2b98058202b927315-0-7\" stroke-width=\"2px\" d=\"M150,202.0 C150,2.0 690.0,2.0 690.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1ceb3b2366048c2b98058202b927315-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M690.0,204.0 L698.0,192.0 682.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1ceb3b2366048c2b98058202b927315-0-8\" stroke-width=\"2px\" d=\"M790,202.0 C790,122.0 915.0,122.0 915.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1ceb3b2366048c2b98058202b927315-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M790,204.0 L782,192.0 798,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1ceb3b2366048c2b98058202b927315-0-9\" stroke-width=\"2px\" d=\"M870,202.0 C870,162.0 910.0,162.0 910.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1ceb3b2366048c2b98058202b927315-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M870,204.0 L862,192.0 878,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1ceb3b2366048c2b98058202b927315-0-10\" stroke-width=\"2px\" d=\"M710,202.0 C710,82.0 920.0,82.0 920.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1ceb3b2366048c2b98058202b927315-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M920.0,204.0 L928.0,192.0 912.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1ceb3b2366048c2b98058202b927315-0-11\" stroke-width=\"2px\" d=\"M950,202.0 C950,162.0 990.0,162.0 990.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1ceb3b2366048c2b98058202b927315-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M990.0,204.0 L998.0,192.0 982.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-b1ceb3b2366048c2b98058202b927315-0-12\" stroke-width=\"2px\" d=\"M1030,202.0 C1030,162.0 1070.0,162.0 1070.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-b1ceb3b2366048c2b98058202b927315-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1070.0,204.0 L1078.0,192.0 1062.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "xpTs0bn5X92D",
        "outputId": "c74d2d11-3424-4b5d-9192-9b49ce2494d2"
      },
      "source": [
        "displacy.render(nlp(text), style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">It took me \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    more than two hours\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
              "</mark>\n",
              " to translate a few pages of \n",
              "<mark class=\"entity\" style=\"background: #ff8197; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    English\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">LANGUAGE</span>\n",
              "</mark>\n",
              ".</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgKt3mTNd15O"
      },
      "source": [
        "doc = nlp(u'Apple is going to build a U.K. factory for $6 million.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5JsyKneeLrd",
        "outputId": "d43659ea-779a-4f5f-83f2-09ae56ba1fc0"
      },
      "source": [
        "for token in doc:\n",
        "  print(token.text,'=>',token.dep_,'=>',token.head.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple => nsubj => going\n",
            "is => aux => going\n",
            "going => ROOT => going\n",
            "to => aux => build\n",
            "build => xcomp => going\n",
            "a => det => factory\n",
            "U.K. => compound => factory\n",
            "factory => dobj => build\n",
            "for => prep => build\n",
            "$ => quantmod => million\n",
            "6 => compound => million\n",
            "million => pobj => for\n",
            ". => punct => going\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "iCGxfDEeeNsY",
        "outputId": "3b4be969-a4a5-4f89-baa9-378b2673f1fb"
      },
      "source": [
        "from spacy import displacy\n",
        "displacy.render(doc,style='dep', jupyter=True, options={'distance': 80})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"8cd648efd40a44a1b71f24d83b80c061-0\" class=\"displacy\" width=\"1010\" height=\"297.0\" direction=\"ltr\" style=\"max-width: none; height: 297.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"130\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"130\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"210\">going</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"210\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">to</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">PART</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"370\">build</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"370\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">U.K.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">factory</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"690\">for</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"690\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">$</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">SYM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">6</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"207.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">million.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">NUM</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8cd648efd40a44a1b71f24d83b80c061-0-0\" stroke-width=\"2px\" d=\"M70,162.0 C70,82.0 200.0,82.0 200.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8cd648efd40a44a1b71f24d83b80c061-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,164.0 L62,152.0 78,152.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8cd648efd40a44a1b71f24d83b80c061-0-1\" stroke-width=\"2px\" d=\"M150,162.0 C150,122.0 195.0,122.0 195.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8cd648efd40a44a1b71f24d83b80c061-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M150,164.0 L142,152.0 158,152.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8cd648efd40a44a1b71f24d83b80c061-0-2\" stroke-width=\"2px\" d=\"M310,162.0 C310,122.0 355.0,122.0 355.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8cd648efd40a44a1b71f24d83b80c061-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M310,164.0 L302,152.0 318,152.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8cd648efd40a44a1b71f24d83b80c061-0-3\" stroke-width=\"2px\" d=\"M230,162.0 C230,82.0 360.0,82.0 360.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8cd648efd40a44a1b71f24d83b80c061-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M360.0,164.0 L368.0,152.0 352.0,152.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8cd648efd40a44a1b71f24d83b80c061-0-4\" stroke-width=\"2px\" d=\"M470,162.0 C470,82.0 600.0,82.0 600.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8cd648efd40a44a1b71f24d83b80c061-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M470,164.0 L462,152.0 478,152.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8cd648efd40a44a1b71f24d83b80c061-0-5\" stroke-width=\"2px\" d=\"M550,162.0 C550,122.0 595.0,122.0 595.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8cd648efd40a44a1b71f24d83b80c061-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M550,164.0 L542,152.0 558,152.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8cd648efd40a44a1b71f24d83b80c061-0-6\" stroke-width=\"2px\" d=\"M390,162.0 C390,42.0 605.0,42.0 605.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8cd648efd40a44a1b71f24d83b80c061-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M605.0,164.0 L613.0,152.0 597.0,152.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8cd648efd40a44a1b71f24d83b80c061-0-7\" stroke-width=\"2px\" d=\"M390,162.0 C390,2.0 690.0,2.0 690.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8cd648efd40a44a1b71f24d83b80c061-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M690.0,164.0 L698.0,152.0 682.0,152.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8cd648efd40a44a1b71f24d83b80c061-0-8\" stroke-width=\"2px\" d=\"M790,162.0 C790,82.0 920.0,82.0 920.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8cd648efd40a44a1b71f24d83b80c061-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M790,164.0 L782,152.0 798,152.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8cd648efd40a44a1b71f24d83b80c061-0-9\" stroke-width=\"2px\" d=\"M870,162.0 C870,122.0 915.0,122.0 915.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8cd648efd40a44a1b71f24d83b80c061-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M870,164.0 L862,152.0 878,152.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-8cd648efd40a44a1b71f24d83b80c061-0-10\" stroke-width=\"2px\" d=\"M710,162.0 C710,42.0 925.0,42.0 925.0,162.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-8cd648efd40a44a1b71f24d83b80c061-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M925.0,164.0 L933.0,152.0 917.0,152.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "VD-y3pRefOa-",
        "outputId": "1c1b3e40-8e36-4bd4-f55f-a0b9a596bf5e"
      },
      "source": [
        "displacy.render(doc, style='ent', jupyter=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Apple\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is going to build a \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.K.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " factory for \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    $6 million\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
              "</mark>\n",
              ".</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5g9_9MthudK"
      },
      "source": [
        "#Part of speech tagging \n",
        "art-of-Speech(POS) Tagging is the process of assigning different labels known as POS tags.which includes nouns, pronouns, adjectives, verbs, etc. Words belonging to various parts of speeches form a sentence. Knowing the part of speech of words in a sentence is important for understanding it.\n",
        "\n",
        "there are two types of POS tags:\n",
        "\n",
        "Universal POS Tags:\n",
        "\n",
        "Detailed POS Tags"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oVwHB3tfVEx",
        "outputId": "caf4ae31-f6be-4c02-ca2b-b437d89fa1cc"
      },
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        " \n",
        "text='It took me more than two hours to translate a few pages of English.'\n",
        "\n",
        "for token in nlp(text):\n",
        " print(token.text, '=>',token.pos_,'=>',token.tag_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It => PRON => PRP\n",
            "took => VERB => VBD\n",
            "me => PRON => PRP\n",
            "more => ADJ => JJR\n",
            "than => SCONJ => IN\n",
            "two => NUM => CD\n",
            "hours => NOUN => NNS\n",
            "to => PART => TO\n",
            "translate => VERB => VB\n",
            "a => DET => DT\n",
            "few => ADJ => JJ\n",
            "pages => NOUN => NNS\n",
            "of => ADP => IN\n",
            "English => PROPN => NNP\n",
            ". => PUNCT => .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5YIvKuCoUCu",
        "outputId": "6ca43fec-096e-4a23-f848-2e5c77beea92"
      },
      "source": [
        "for token in nlp(text):\n",
        "    print(token.text, token.pos_, token.dep_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "It PRON nsubj\n",
            "took VERB ROOT\n",
            "me PRON dative\n",
            "more ADJ amod\n",
            "than SCONJ quantmod\n",
            "two NUM nummod\n",
            "hours NOUN dobj\n",
            "to PART aux\n",
            "translate VERB xcomp\n",
            "a DET det\n",
            "few ADJ amod\n",
            "pages NOUN dobj\n",
            "of ADP prep\n",
            "English PROPN pobj\n",
            ". PUNCT punct\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WCeNpXzMomhe",
        "outputId": "c5eb4fe8-46ba-461c-8943-60a5aeeae5c0"
      },
      "source": [
        "spacy.explain('PROPN')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'proper noun'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_BvwCo-voqOa",
        "outputId": "c53d5442-a79c-4fc7-fa2e-250c58cbe2d4"
      },
      "source": [
        "spacy.explain('nsubj')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'nominal subject'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YANz5yobpEux"
      },
      "source": [
        "#Additional Token Attributes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "netImO8VhuyG"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UO0EZJWtpBEW",
        "outputId": "f9ec2eea-e2b4-4b52-858d-c91974ed4176"
      },
      "source": [
        "doc2 = nlp(u\"We're here to help! Send snail-mail, email support@oursite.com or visit us at http://www.oursite.com!\")\n",
        "\n",
        "for token in doc2:\n",
        " print(token.text, '=>',token.pos_,'=>',token.tag_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We => PRON => PRP\n",
            "'re => AUX => VBP\n",
            "here => ADV => RB\n",
            "to => PART => TO\n",
            "help => VERB => VB\n",
            "! => PUNCT => .\n",
            "Send => VERB => VB\n",
            "snail => NOUN => NN\n",
            "- => PUNCT => HYPH\n",
            "mail => NOUN => NN\n",
            ", => PUNCT => ,\n",
            "email => NOUN => NN\n",
            "support@oursite.com => X => ADD\n",
            "or => CCONJ => CC\n",
            "visit => VERB => VB\n",
            "us => PRON => PRP\n",
            "at => ADP => IN\n",
            "http://www.oursite.com => X => ADD\n",
            "! => PUNCT => .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqOSmi9eq-F9",
        "outputId": "dc1eb291-4d03-4634-d1b8-0572d7c50b37"
      },
      "source": [
        "from nltk import word_tokenize, pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EloWsEmlqjT2",
        "outputId": "1b27b2cb-f334-4f79-e807-f6ede0d10431"
      },
      "source": [
        "text = \"I am learning Natural Language Processing on Analytics Vidhya\"\n",
        "tokens = word_tokenize(text)\n",
        "print (pos_tag(tokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('on', 'IN'), ('Analytics', 'NNP'), ('Vidhya', 'NNP')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkbulZzWq056"
      },
      "source": [
        "#Named-entity recognition (NER)\n",
        "is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLCaoz96ud-x"
      },
      "source": [
        "also known as entity chunking/extraction , is a popular technique used in information extraction to identify and segment the named entities and classify or categorize them under various predefined classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ODaDmhXx3Rix"
      },
      "source": [
        "[Named-entity recognition extraction method](https://www.kdnuggets.com/2018/08/named-entity-recognition-practitioners-guide-nlp-4.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKfTRi_ludHG",
        "outputId": "6415d306-ba27-4222-aba2-6b278425b8ee"
      },
      "source": [
        "\n",
        "doc8 = nlp(u'Apple to build a Hong Kong factory for $6 million')\n",
        "\n",
        "for token in doc8:\n",
        "    print(token.text, end=' | ')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple | to | build | a | Hong | Kong | factory | for | $ | 6 | million | "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULVG3FD3YidB",
        "outputId": "58334425-26e6-4bfb-e50e-d2fb0f41957c"
      },
      "source": [
        "for entity in doc8.ents:\n",
        "    print(entity)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple\n",
            "Hong Kong\n",
            "$6 million\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8AF95faOZaqv",
        "outputId": "3ddd0b31-2f92-424f-99a3-315085e8d6d7"
      },
      "source": [
        "for ent in doc8.ents:\n",
        "    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple - ORG - Companies, agencies, institutions, etc.\n",
            "Hong Kong - GPE - Countries, cities, states\n",
            "$6 million - MONEY - Monetary values, including unit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmZULl9aZdk6",
        "outputId": "5194d12b-82a1-4d2f-d3d8-d9d7028f5e49"
      },
      "source": [
        "for entity in doc8.ents:\n",
        "    print(entity)\n",
        "    print(entity.label_)\n",
        "    print(spacy.explain(entity.label_))\n",
        "    print('\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple\n",
            "ORG\n",
            "Companies, agencies, institutions, etc.\n",
            "\n",
            "\n",
            "Hong Kong\n",
            "GPE\n",
            "Countries, cities, states\n",
            "\n",
            "\n",
            "$6 million\n",
            "MONEY\n",
            "Monetary values, including unit\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7IPfCsZiZlYb",
        "outputId": "93131eb2-f644-49cc-a07f-8be6b72673d9"
      },
      "source": [
        "import spacy \n",
        "  \n",
        "nlp = spacy.load('en_core_web_sm') \n",
        "  \n",
        "sentence = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
        "  \n",
        "doc = nlp(sentence) \n",
        "  \n",
        "for ent in doc.ents: \n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apple 0 5 ORG\n",
            "U.K. 27 31 GPE\n",
            "$1 billion 44 54 MONEY\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5MpaeQ_bTaD"
      },
      "source": [
        "In the output, the first column specifies the entity, the next two columns the start and end characters within the sentence/document, and the final column specifies the category."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKYD8fkbklL7"
      },
      "source": [
        "#Noun Chunks\n",
        "Similar to Doc.ents, Doc.noun_chunks are another object property. Noun chunks are \"base noun phrases\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OyiccR7bDyT",
        "outputId": "4f3b6181-abb2-4d9f-b92e-2c7649a1918a"
      },
      "source": [
        "doc9 = nlp(u\"Autonomous cars shift insurance liability toward manufacturers.\")\n",
        "\n",
        "for chunk in doc9.noun_chunks:\n",
        "    print(chunk.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Autonomous cars\n",
            "insurance liability\n",
            "manufacturers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XHcjkK1df3L",
        "outputId": "4e0e534a-ec76-41f0-cd06-84620a8c34f7"
      },
      "source": [
        "doc10 = nlp(u\"Red cars do not carry higher insurance rates.\")\n",
        "\n",
        "for chunk in doc10.noun_chunks:\n",
        "    print(chunk.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Red cars\n",
            "higher insurance rates\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PJP_uiRzd5dA",
        "outputId": "e41a8aa1-7436-4446-8a77-6276228f5c78"
      },
      "source": [
        "doc11 = nlp(u\"He was a one-eyed, one-horned, flying, purple people-eater.\")\n",
        "\n",
        "for chunk in doc11.noun_chunks:\n",
        "    print(chunk.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "He\n",
            "a one-eyed, one-horned, flying, purple people-eater\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9ypPK2p_7z2"
      },
      "source": [
        "# Add new entity\n",
        "Normally we would have spaCy build a library of named entities by training it on several samples of text.\n",
        "In this case, we only want to add one value: \n",
        "\n",
        "some time few text not specifed the entity. so we can assinge the entity tothe text\n",
        "\n",
        "example is bellow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQSFawc0BEfb"
      },
      "source": [
        "# Write a function to display basic entity info:\n",
        "def show_ents(doc):\n",
        "    if doc.ents:\n",
        "        for ent in doc.ents:\n",
        "            print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))\n",
        "    else:\n",
        "        print('No named entities found.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZyamHZrea-9",
        "outputId": "4fa9b1e6-18b7-4480-ae47-42c4d2daf9d9"
      },
      "source": [
        "doc = nlp(u'Tesla to build a U.K. factory for $6 million')\n",
        "\n",
        "show_ents(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "U.K. - GPE - Countries, cities, states\n",
            "$6 million - MONEY - Monetary values, including unit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QNowCtxBJ75"
      },
      "source": [
        "Right now, spaCy does not recognize \"Tesla\" as a company."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0QLYe9KAsFL"
      },
      "source": [
        "from spacy.tokens import Span\n",
        "\n",
        "# Get the hash value of the ORG entity label\n",
        "ORG = doc.vocab.strings[u'ORG']  \n",
        "\n",
        "# Create a Span for the new entity\n",
        "new_ent = Span(doc, 0, 1, label=ORG)\n",
        "\n",
        "# Add the entity to the existing Doc object\n",
        "doc.ents = list(doc.ents) + [new_ent]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrCayd-PBUQd"
      },
      "source": [
        "In the code above, the arguments passed to Span() are:\n",
        "\n",
        "doc - the name of the Doc object\n",
        "\n",
        "0 - the start index position of the span\n",
        "\n",
        "1 - the stop index position (exclusive)\n",
        "\n",
        "label=ORG - the label assigned to our entity\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T0Nf-JYNBNt3",
        "outputId": "53bbaf76-dcef-43a1-8a73-d5214709f42d"
      },
      "source": [
        "show_ents(doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tesla - ORG - Companies, agencies, institutions, etc.\n",
            "U.K. - GPE - Countries, cities, states\n",
            "$6 million - MONEY - Monetary values, including unit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjuLoTL-Cplg"
      },
      "source": [
        "#Sentence Segmentation\n",
        "\n",
        "In spaCy Basics we saw briefly how Doc objects are divided into sentences. In this section we'll learn how sentence segmentation works, and how to set our own segmentation rules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hY74yV1OBRgJ",
        "outputId": "e9aea24a-6997-4480-b6fc-4fc81fde8cac"
      },
      "source": [
        "# From Spacy Basics:\n",
        "doc = nlp(u'This is the first sentence. This is another sentence. This is the last sentence.')\n",
        "\n",
        "for sent in doc.sents:\n",
        "    print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is the first sentence.\n",
            "This is another sentence.\n",
            "This is the last sentence.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OT9Um_KdxjI"
      },
      "source": [
        "Doc.sents is a generator\n",
        "It is important to note that doc.sents is a generator. That is, a Doc is not segmented until doc.sents is called. This means that, where you could print the second Doc token with print(doc[1]), you can't call the \"second Doc sentence\" with print(doc.sents[1]):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4m-MedA9a2wF",
        "outputId": "d2761195-25dc-45bc-d0e6-40b836f32ec9"
      },
      "source": [
        "print(doc[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "is\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "id": "jPGYEc_ld-fO",
        "outputId": "1d528e19-89f3-4b59-c7ed-8a3cad4758af"
      },
      "source": [
        "print(doc.sents[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-2bc012eee1da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nruFZmUdeJUe"
      },
      "source": [
        "However, you *can* build a sentence collection by running `doc.sents` and saving the result to a list:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOm7xrKXeABq",
        "outputId": "b7e59e5a-c0c0-4a47-ac56-7b684fe005de"
      },
      "source": [
        "doc_sents = [sent for sent in doc.sents]\n",
        "doc_sents"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[This is the first sentence.,\n",
              " This is another sentence.,\n",
              " This is the last sentence.]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6zFYMxJeRrm"
      },
      "source": [
        "NOTE: list(doc.sents) also works. We show a list comprehension as it allows you to pass in conditionals."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZoN_qPLeLwo",
        "outputId": "0e60ebd4-b469-43f9-8335-ace73450bda4"
      },
      "source": [
        "# Now you can access individual sentences:\n",
        "print(doc_sents[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is another sentence.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbX2Cc1TeVNt",
        "outputId": "055a2e18-b3ca-49fd-c80f-de7be7d4a00b"
      },
      "source": [
        "type(doc_sents[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.span.Span"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGLPflTJejV2",
        "outputId": "4d90114a-8e27-4b9a-d943-c4d7f86e7d36"
      },
      "source": [
        "print(doc_sents[1].start, doc_sents[1].end)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6 11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymX0hfDtfvYn"
      },
      "source": [
        "Let's add a semicolon to our existing segmentation rules. That is, whenever the sentencizer encounters a semicolon, the next token should start a new segment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2vJ78BhfwxM",
        "outputId": "e6877b1c-837f-47f6-a2e4-493f5246cd49"
      },
      "source": [
        "# SPACY'S DEFAULT BEHAVIOR\n",
        "doc3 = nlp(u'\"Management is doing things right; leadership is doing the right things.\" -Peter Drucker')\n",
        "\n",
        "for sent in doc3.sents:\n",
        "    print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Management is doing things right; leadership is doing the right things.\"\n",
            "-Peter\n",
            "Drucker\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcYedbwSelK-",
        "outputId": "f6905c29-5992-45f1-db7d-3ca3c311b337"
      },
      "source": [
        "nlp.pipe_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger', 'parser', 'ner']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHSCFO3Rf8nD"
      },
      "source": [
        "# ADD A NEW RULE TO THE PIPELINE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DfjQPBFpfw-K",
        "outputId": "743a5a01-9456-414a-fd5a-cfcc9a7a7be6"
      },
      "source": [
        "# ADD A NEW RULE TO THE PIPELINE\n",
        "def set_custom_boundaries(doc):\n",
        "    for token in doc[:-1]:\n",
        "        if token.text == ';':\n",
        "            doc[token.i+1].is_sent_start = True\n",
        "    return doc\n",
        "\n",
        "nlp.add_pipe(set_custom_boundaries, before='parser')\n",
        "\n",
        "nlp.pipe_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tagger', 'set_custom_boundaries', 'parser', 'ner']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcyXczo4ghzU"
      },
      "source": [
        "<font color=green>The new rule has to run before the document is parsed. Here we can either pass the argument `before='parser'` or `first=True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnXsdPATfiVj",
        "outputId": "d772a5ee-d0bf-454c-c916-dc68f95f43a3"
      },
      "source": [
        "# Re-run the Doc object creation:\n",
        "doc4 = nlp(u'\"Management is doing things right; leadership is doing the right things.\" -Peter Drucker')\n",
        "\n",
        "for sent in doc4.sents:\n",
        "    print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Management is doing things right;\n",
            "leadership is doing the right things.\"\n",
            "-Peter\n",
            "Drucker\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYBn8SuqgCQK",
        "outputId": "f4fed675-05be-46a0-9210-2542da64b38c"
      },
      "source": [
        "# And yet the new rule doesn't apply to the older Doc object:\n",
        "for sent in doc3.sents:\n",
        "    print(sent)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\"Management is doing things right; leadership is doing the right things.\"\n",
            "-Peter\n",
            "Drucker\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgDcFJJYgHna"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rdyG9mHh5Jp"
      },
      "source": [
        "#[Vocabulary and Matching](https://notebook.community/rishuatgithub/MLPy/nlp/UPDATED_NLP_COURSE/01-NLP-Python-Basics/05-Vocabulary-and-Matching)\n",
        "\n",
        "So far we've seen how a body of text is divided into tokens, and how individual tokens are parsed and tagged with parts of speech, dependencies and lemmas.\n",
        "\n",
        "In this section we will identify and label specific phrases that match patterns we can define ourselves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utj94GA7kgwR"
      },
      "source": [
        "spaCy offers a rule-matching tool called Matcher that allows you to build a library of token patterns, then match those patterns against a Doc object to return a list of found matches. You can match on any part of the token including text and annotations, and you can add multiple patterns to the same matcher."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7z9CJRsh6Uj"
      },
      "source": [
        "# Perform standard imports\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UIfrs_eAklLO"
      },
      "source": [
        "# Import the Matcher library\n",
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTwdmFlckp2S"
      },
      "source": [
        "#Creating patterns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDB75zm2knCU"
      },
      "source": [
        "pattern1 = [{'LOWER': 'solarpower'}]\n",
        "pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\n",
        "pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\n",
        "\n",
        "matcher.add('SolarPower', None, pattern1, pattern2, pattern3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQZMSMV5lbLV"
      },
      "source": [
        "Let's break this down:\n",
        "\n",
        "pattern1 looks for a single token whose lowercase text reads 'solarpower'\n",
        "\n",
        "pattern2 looks for two adjacent tokens that read 'solar' and 'power' in that order\n",
        "\n",
        "pattern3 looks for three adjacent tokens, with a middle token that can be any punctuation.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqVHk8YvlRZQ",
        "outputId": "e9da355f-0ade-4dcd-b81b-e5cd5c2c7291"
      },
      "source": [
        "#Applying the matcher to a Doc object\n",
        "doc = nlp(u'The Solar Power industry continues to grow as demand \\\n",
        "for solarpower increases. Solar-power cars are gaining popularity.')\n",
        "\n",
        "found_matches = matcher(doc)\n",
        "print(found_matches)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvMf2FDdmGRd",
        "outputId": "3f7ce544-1c25-4d3b-b20f-7ff107a2864e"
      },
      "source": [
        "for match_id, start, end in found_matches:\n",
        "    string_id = nlp.vocab.strings[match_id]  # get string representation\n",
        "    span = doc[start:end]                    # get the matched span\n",
        "    print(match_id, string_id, start, end, span.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8656102463236116519 SolarPower 1 3 Solar Power\n",
            "8656102463236116519 SolarPower 10 11 solarpower\n",
            "8656102463236116519 SolarPower 13 16 Solar-power\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8MMqMQ0qGGw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S42FmN6-7uH6"
      },
      "source": [
        "[Text processing sample code](https://notebook.community/Mashimo/datascience/03-NLP/helloworld-nlp)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ev54P7Ru750I"
      },
      "source": [
        "sampleText = \" The Elephant's 4 legs: this is THE Pub. Hi, you, my super_friend! You can't believe what happened to our * common friend *, the butcher. How are you?\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTFosjhr8WGR"
      },
      "source": [
        "Tokens can be extracted by splitting the text:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oG1N92lI8GTm"
      },
      "source": [
        "textTokens = sampleText.split() # by default split by spaces"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JyK5xLtT8g88",
        "outputId": "409f4595-7e2d-479e-b8e4-6f1e05b300d0"
      },
      "source": [
        "textTokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The',\n",
              " \"Elephant's\",\n",
              " '4',\n",
              " 'legs:',\n",
              " 'this',\n",
              " 'is',\n",
              " 'THE',\n",
              " 'Pub.',\n",
              " 'Hi,',\n",
              " 'you,',\n",
              " 'my',\n",
              " 'super_friend!',\n",
              " 'You',\n",
              " \"can't\",\n",
              " 'believe',\n",
              " 'what',\n",
              " 'happened',\n",
              " 'to',\n",
              " 'our',\n",
              " '*',\n",
              " 'common',\n",
              " 'friend',\n",
              " '*,',\n",
              " 'the',\n",
              " 'butcher.',\n",
              " 'How',\n",
              " 'are',\n",
              " 'you?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ruvin3eA8jAN",
        "outputId": "7beb7b7e-87b6-4f25-ee9e-9af6b652d5c1"
      },
      "source": [
        "print (\"The sample text has {} tokens\".format (len(textTokens)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The sample text has 28 tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCbXCL-s8040"
      },
      "source": [
        "As you can see, tokens are words but also symbols like *. This is because we have split the string simply by using blank spaces. But you can pass other separators to the function split() such as commas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BUUzD5Q84sf"
      },
      "source": [
        "# Tokens frequency\n",
        "I have the number of tokens for this sample text. Let's say now that I want to count the frequency for each token.\n",
        "This can be done quickly by using the Python package Counter from collections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xl4jYM338oRS",
        "outputId": "59e59264-96bb-43a5-97c4-711a36abc517"
      },
      "source": [
        "from collections import Counter\n",
        "totalWords = Counter(textTokens); \n",
        "totalWords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'*': 1,\n",
              "         '*,': 1,\n",
              "         '4': 1,\n",
              "         \"Elephant's\": 1,\n",
              "         'Hi,': 1,\n",
              "         'How': 1,\n",
              "         'Pub.': 1,\n",
              "         'THE': 1,\n",
              "         'The': 1,\n",
              "         'You': 1,\n",
              "         'are': 1,\n",
              "         'believe': 1,\n",
              "         'butcher.': 1,\n",
              "         \"can't\": 1,\n",
              "         'common': 1,\n",
              "         'friend': 1,\n",
              "         'happened': 1,\n",
              "         'is': 1,\n",
              "         'legs:': 1,\n",
              "         'my': 1,\n",
              "         'our': 1,\n",
              "         'super_friend!': 1,\n",
              "         'the': 1,\n",
              "         'this': 1,\n",
              "         'to': 1,\n",
              "         'what': 1,\n",
              "         'you,': 1,\n",
              "         'you?': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8Eu-seJ9Jch"
      },
      "source": [
        "There are a number of problems:\n",
        "\n",
        "some word (like The/THE or You/you) is in two different tokens because of capital vs. small letter\n",
        "\n",
        "some token contains punctuation marks which makes the same word counted twice\n",
        "\n",
        "same token consists of symbols (like *)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PPP8I_J9P8L"
      },
      "source": [
        "#Remove capital letters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "Ve7Mvl6o9EsY",
        "outputId": "6afdcdfa-98b3-46bd-a0e3-834a2f88473a"
      },
      "source": [
        "loweredText = sampleText.lower()\n",
        "loweredText"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" the elephant's 4 legs: this is the pub. hi, you, my super_friend! you can't believe what happened to our * common friend *, the butcher. how are you?\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecYsnF8V9TIi",
        "outputId": "cdd8eefe-16cf-4d1c-9ec3-f889820e9e10"
      },
      "source": [
        "textTokens = loweredText.split()\n",
        "totalWords = Counter(textTokens); \n",
        "totalWords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'*': 1,\n",
              "         '*,': 1,\n",
              "         '4': 1,\n",
              "         'are': 1,\n",
              "         'believe': 1,\n",
              "         'butcher.': 1,\n",
              "         \"can't\": 1,\n",
              "         'common': 1,\n",
              "         \"elephant's\": 1,\n",
              "         'friend': 1,\n",
              "         'happened': 1,\n",
              "         'hi,': 1,\n",
              "         'how': 1,\n",
              "         'is': 1,\n",
              "         'legs:': 1,\n",
              "         'my': 1,\n",
              "         'our': 1,\n",
              "         'pub.': 1,\n",
              "         'super_friend!': 1,\n",
              "         'the': 3,\n",
              "         'this': 1,\n",
              "         'to': 1,\n",
              "         'what': 1,\n",
              "         'you': 1,\n",
              "         'you,': 1,\n",
              "         'you?': 1})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVNAx43p9k4p"
      },
      "source": [
        "Now the token \"the\" is counted correctly 3 times !\n",
        "But other words like you are still wrongly counted because of the punctuation such as comma or question mark."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6mm4ALp9oDn"
      },
      "source": [
        "#Remove punctuation and trailing spaces\n",
        "\n",
        "Removing the extra spaces is very easy, by using the string function strip():"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "1soMRxUo9ev4",
        "outputId": "dab2745e-7ecf-47c4-bb0f-e6315d451b88"
      },
      "source": [
        "strippedText = loweredText.strip()\n",
        "strippedText"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"the elephant's 4 legs: this is the pub. hi, you, my super_friend! you can't believe what happened to our * common friend *, the butcher. how are you?\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDpxbtKUAE4s"
      },
      "source": [
        "To remove punctuaction we can use regular expressions.\n",
        "Regular expression are very powerful for match patterns in a sequence of characters.\n",
        "\n",
        "The way in RE to match specific characters is to list them inside square brackets. For example [abc] will match only a single a,b or c letter an nothing else. A shorthand for matching sequential characters is to use the dash, for example [a-z] will match any single lowercase letter and [0-9] any single digit character.\n",
        "\n",
        "To exclude characters from the matching we use the ^ (hat) symbol, for example [^abc] will match any single character except the letters a, b or c.\n",
        "\n",
        "Finally there is also a special symbol for the whitespaces as they are so ubiquitous in text. Whitespaces are blank spaces, tabs, newlines, carriage returns. The symbol \\s will match any of them.\n",
        "\n",
        "The function re.sub() takes as input a starting string, a string to substitute and a pattern to match and returns the string with the matches replaced with the given substring using the given pattern. for example re.sub(r'[s]', 'z', \"whatsapp\") will return \"whatzapp\".\n",
        "\n",
        "So one way to remove the punctuation is to replace any character that is NOT a letter, a number or a whitespace with an empty substring:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbfWtuXp9y94"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aTqe1eL0AQrf",
        "outputId": "6c8a1ec9-0beb-428e-949c-e9da98dc7533"
      },
      "source": [
        "processedText = re.sub(r'[^a-z0-9\\s]', '', strippedText) # keep only numbers and letters\n",
        "processedText"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the elephants 4 legs this is the pub hi you my superfriend you cant believe what happened to our  common friend  the butcher how are you'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCZYY_QmBEBi"
      },
      "source": [
        "Another useful symbol is \\w which match ANY alphanumeric character or \\W which matches any NON alphanumeric character.\n",
        "So, an alternative way could be"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "aFNbXtpFA7o0",
        "outputId": "a516aa9c-4908-4134-9ec6-be071cd0fff7"
      },
      "source": [
        "processedText = re.sub(r'[^\\s\\w]', '', strippedText) # remove punctuation\n",
        "processedText"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the elephants 4 legs this is the pub hi you my super_friend you cant believe what happened to our  common friend  the butcher how are you'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Oo4b_jPqBBut",
        "outputId": "17b4631e-b094-4c9f-c252-2cf0530a6ebd"
      },
      "source": [
        "# alternative way to remove spaces at the beginning\n",
        "processedText4 = re.sub(r'^\\s+', r'', processedText) \n",
        "processedText4"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the elephants 4 legs this is the pub hi you my super_friend you cant believe what happened to our  common friend  the butcher how are you'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iSrowKFEBbfn",
        "outputId": "1a6b07ed-0355-4cab-c9e3-46aaab643068"
      },
      "source": [
        "textTokens = processedText.split()\n",
        "totalWords = Counter(textTokens); \n",
        "totalWords"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'4': 1,\n",
              "         'are': 1,\n",
              "         'believe': 1,\n",
              "         'butcher': 1,\n",
              "         'cant': 1,\n",
              "         'common': 1,\n",
              "         'elephants': 1,\n",
              "         'friend': 1,\n",
              "         'happened': 1,\n",
              "         'hi': 1,\n",
              "         'how': 1,\n",
              "         'is': 1,\n",
              "         'legs': 1,\n",
              "         'my': 1,\n",
              "         'our': 1,\n",
              "         'pub': 1,\n",
              "         'super_friend': 1,\n",
              "         'the': 3,\n",
              "         'this': 1,\n",
              "         'to': 1,\n",
              "         'what': 1,\n",
              "         'you': 3})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxoL_KAuBkoC"
      },
      "source": [
        "Now the token you is also counted correctly 3 times.\n",
        "\n",
        "A collection can be sorted easily"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foe6xtqABgFG",
        "outputId": "90ad78a8-0533-4df2-fa31-9e08047edeaf"
      },
      "source": [
        "print (totalWords.items())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_items([('the', 3), ('elephants', 1), ('4', 1), ('legs', 1), ('this', 1), ('is', 1), ('pub', 1), ('hi', 1), ('you', 3), ('my', 1), ('super_friend', 1), ('cant', 1), ('believe', 1), ('what', 1), ('happened', 1), ('to', 1), ('our', 1), ('common', 1), ('friend', 1), ('butcher', 1), ('how', 1), ('are', 1)])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcEb77hJBpel",
        "outputId": "eb597770-5558-49d9-bf6b-1fbaadaf69f6"
      },
      "source": [
        "sorted(totalWords.items(), key=lambda x:x[1],reverse=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 3),\n",
              " ('you', 3),\n",
              " ('elephants', 1),\n",
              " ('4', 1),\n",
              " ('legs', 1),\n",
              " ('this', 1),\n",
              " ('is', 1),\n",
              " ('pub', 1),\n",
              " ('hi', 1),\n",
              " ('my', 1),\n",
              " ('super_friend', 1),\n",
              " ('cant', 1),\n",
              " ('believe', 1),\n",
              " ('what', 1),\n",
              " ('happened', 1),\n",
              " ('to', 1),\n",
              " ('our', 1),\n",
              " ('common', 1),\n",
              " ('friend', 1),\n",
              " ('butcher', 1),\n",
              " ('how', 1),\n",
              " ('are', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE6kfISYB1Ve"
      },
      "source": [
        "An alternative way (without lambda functions) to sort a collection:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiWJDBcSBvml",
        "outputId": "a8423b13-7318-4a3b-f080-f69d083e202b"
      },
      "source": [
        "from operator import itemgetter\n",
        "sorted(totalWords.items(), key=itemgetter(1), reverse=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 3),\n",
              " ('you', 3),\n",
              " ('elephants', 1),\n",
              " ('4', 1),\n",
              " ('legs', 1),\n",
              " ('this', 1),\n",
              " ('is', 1),\n",
              " ('pub', 1),\n",
              " ('hi', 1),\n",
              " ('my', 1),\n",
              " ('super_friend', 1),\n",
              " ('cant', 1),\n",
              " ('believe', 1),\n",
              " ('what', 1),\n",
              " ('happened', 1),\n",
              " ('to', 1),\n",
              " ('our', 1),\n",
              " ('common', 1),\n",
              " ('friend', 1),\n",
              " ('butcher', 1),\n",
              " ('how', 1),\n",
              " ('are', 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7asxfvz-CA0z"
      },
      "source": [
        "# Let's put these results into functions we can re-use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kairiXCZCcy4"
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "from operator import itemgetter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtU2XNkPB4ar"
      },
      "source": [
        "def tokenise(text):\n",
        "    return text.split() # split by space; return a list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-H0HsFcECTxQ"
      },
      "source": [
        "def removePunctuation(text):\n",
        "    processedText = re.sub(r'([^\\s\\w_]|_)+', '', text.strip()) # remove punctuation\n",
        "\n",
        "    return processedText"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZP8qMeZCLBB"
      },
      "source": [
        "def preprocessText(text, lowercase=True):\n",
        "    if lowercase:\n",
        "        processedText = removePunctuation(text.lower())\n",
        "    else:\n",
        "        processedText = removePunctuation(text)\n",
        "\n",
        "    return tokenise(processedText)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kP8nlV7yCaYq"
      },
      "source": [
        "def getMostCommonWords(tokens, n=10):\n",
        "    wordsCount = Counter(tokens) # count the occurrences\n",
        "    \n",
        "    return wordsCount.most_common()[:n]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yi32fZEDwlV"
      },
      "source": [
        "# Let's process a text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LezPOx-1Dy40",
        "outputId": "c5538406-6887-44e5-ba0c-eece4b898da4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfmMVTifFs-O"
      },
      "source": [
        "filePath = \"/content/drive/MyDrive/NLP-Natural-Language-Processing-Methods/halfgirlfriend.txt\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eENkqfJFe8E"
      },
      "source": [
        "f = open(filePath,encoding='cp1252')\n",
        "try:\n",
        "    theText = f.read()  # this is a giant String\n",
        "finally:\n",
        "    f.close()  # we should always close the file once finished"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W0ZJAOKGC97",
        "outputId": "791b288d-4c26-4c8e-bce0-c7c8949c8c21"
      },
      "source": [
        "print (\"*** Analysing Novel text:\")  \n",
        "print (\"The book is {} chars long\".format (len(theText)))\n",
        "\n",
        "tokens = preprocessText(theText)\n",
        "\n",
        "print (\"The text has {} tokens\".format (len(tokens)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "*** Analysing Novel text:\n",
            "The book is 18418 chars long\n",
            "The text has 3286 tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oX6AhFTHNsK"
      },
      "source": [
        "A better way is to read the file line by line.\n",
        "We can do this with a simple loop which will go through each line.\n",
        "Note that the block keyword with will automatically close the file at the end of the block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVCNZPVwG7mw",
        "outputId": "ea055ef3-aee6-4596-97a8-ce5c977c3b25"
      },
      "source": [
        "lines = 0\n",
        "\n",
        "with open(filePath,encoding='cp1252') as f:\n",
        "    for line in f:\n",
        "               # I know that the very first line is the book title\n",
        "        if lines == 0:\n",
        "            print (\"Title: {}\".format(line))\n",
        "            \n",
        "               # every line gets processed\n",
        "        lineTokens = preprocessText(line)               \n",
        "                # append the tokens to my list\n",
        "        textTokens.extend(lineTokens)\n",
        "        \n",
        "        lines += 1  # finally move to the next line"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Title: Abstract \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVfXOFZ7HRoD",
        "outputId": "f19e4bd3-b6bf-44ca-dcc2-cc383d5e09be"
      },
      "source": [
        "print (\"The text has {} lines\".format (lines))\n",
        "print (\"The text has {} tokens\".format (len(textTokens)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The text has 250 lines\n",
            "The text has 3312 tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7TAkxEfHkoD",
        "outputId": "56500ee9-f3da-442e-9423-3c414cface52"
      },
      "source": [
        "textTokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the',\n",
              " 'elephants',\n",
              " '4',\n",
              " 'legs',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'pub',\n",
              " 'hi',\n",
              " 'you',\n",
              " 'my',\n",
              " 'super_friend',\n",
              " 'you',\n",
              " 'cant',\n",
              " 'believe',\n",
              " 'what',\n",
              " 'happened',\n",
              " 'to',\n",
              " 'our',\n",
              " 'common',\n",
              " 'friend',\n",
              " 'the',\n",
              " 'butcher',\n",
              " 'how',\n",
              " 'are',\n",
              " 'you',\n",
              " 'abstract',\n",
              " 'chetan',\n",
              " 'bhagat',\n",
              " 'is',\n",
              " 'the',\n",
              " 'novelist',\n",
              " 'whose',\n",
              " 'area',\n",
              " 'of',\n",
              " 'concern',\n",
              " 'is',\n",
              " 'the',\n",
              " 'length',\n",
              " 'and',\n",
              " 'breadth',\n",
              " 'of',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'country',\n",
              " 'especially',\n",
              " 'of',\n",
              " 'the',\n",
              " 'urban',\n",
              " 'areas',\n",
              " 'he',\n",
              " 'uses',\n",
              " 'different',\n",
              " 'techniques',\n",
              " 'in',\n",
              " 'his',\n",
              " 'writings',\n",
              " 'to',\n",
              " 'write',\n",
              " 'about',\n",
              " 'the',\n",
              " 'india',\n",
              " 'he',\n",
              " 'writes',\n",
              " 'to',\n",
              " 'free',\n",
              " 'india',\n",
              " 'from',\n",
              " 'the',\n",
              " 'conservative',\n",
              " 'attitudes',\n",
              " 'of',\n",
              " 'the',\n",
              " 'people',\n",
              " 'and',\n",
              " 'false',\n",
              " 'myths',\n",
              " 'he',\n",
              " 'hopes',\n",
              " 'through',\n",
              " 'his',\n",
              " 'writings',\n",
              " 'to',\n",
              " 'dissolve',\n",
              " 'boundaries',\n",
              " 'between',\n",
              " 'people',\n",
              " 'of',\n",
              " 'different',\n",
              " 'backgrounds',\n",
              " 'cultures',\n",
              " 'communities',\n",
              " 'and',\n",
              " 'different',\n",
              " 'spaces',\n",
              " 'the',\n",
              " 'writers',\n",
              " 'of',\n",
              " 'the',\n",
              " 'country',\n",
              " 'tried',\n",
              " 'their',\n",
              " 'best',\n",
              " 'to',\n",
              " 'explore',\n",
              " 'the',\n",
              " 'struggles',\n",
              " 'and',\n",
              " 'the',\n",
              " 'challenges',\n",
              " 'of',\n",
              " 'new',\n",
              " 'generation',\n",
              " 'but',\n",
              " 'chetan',\n",
              " 'bhagat',\n",
              " 'is',\n",
              " 'chiefly',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'writers',\n",
              " 'whose',\n",
              " 'novels',\n",
              " 'are',\n",
              " 'the',\n",
              " 'representation',\n",
              " 'of',\n",
              " 'such',\n",
              " 'issues',\n",
              " 'the',\n",
              " 'novel',\n",
              " 'half',\n",
              " 'girlfriend',\n",
              " 'portrays',\n",
              " 'the',\n",
              " 'true',\n",
              " 'picture',\n",
              " 'of',\n",
              " 'indian',\n",
              " 'society',\n",
              " 'in',\n",
              " 'contemporary',\n",
              " 'times',\n",
              " 'the',\n",
              " 'paper',\n",
              " 'explores',\n",
              " 'the',\n",
              " 'struggles',\n",
              " 'of',\n",
              " 'the',\n",
              " 'characters',\n",
              " 'and',\n",
              " 'how',\n",
              " 'they',\n",
              " 'pass',\n",
              " 'through',\n",
              " 'such',\n",
              " 'hardships',\n",
              " 'to',\n",
              " 'get',\n",
              " 'success',\n",
              " 'keywords',\n",
              " 'multiculturalism',\n",
              " 'new',\n",
              " 'generation',\n",
              " 'inferiority',\n",
              " 'complex',\n",
              " 'individuality',\n",
              " 'cheatn',\n",
              " 'bhagat',\n",
              " 'is',\n",
              " 'such',\n",
              " 'a',\n",
              " 'novelist',\n",
              " 'who',\n",
              " 'expresses',\n",
              " 'his',\n",
              " 'profound',\n",
              " 'love',\n",
              " 'for',\n",
              " 'his',\n",
              " 'country',\n",
              " 'through',\n",
              " 'his',\n",
              " 'writing',\n",
              " 'he',\n",
              " 'portrays',\n",
              " 'such',\n",
              " 'issues',\n",
              " 'in',\n",
              " 'his',\n",
              " 'writing',\n",
              " 'that',\n",
              " 'are',\n",
              " 'very',\n",
              " 'close',\n",
              " 'to',\n",
              " 'modern',\n",
              " 'generation',\n",
              " 'he',\n",
              " 'comments',\n",
              " 'on',\n",
              " 'faulty',\n",
              " 'education',\n",
              " 'system',\n",
              " 'communal',\n",
              " 'and',\n",
              " 'racial',\n",
              " 'society',\n",
              " 'problems',\n",
              " 'of',\n",
              " 'indian',\n",
              " 'youth',\n",
              " 'and',\n",
              " 'so',\n",
              " 'on',\n",
              " 'therefore',\n",
              " 'it',\n",
              " 'is',\n",
              " 'necessary',\n",
              " 'to',\n",
              " 'indian',\n",
              " 'todays',\n",
              " 'comments',\n",
              " 'regarding',\n",
              " 'him',\n",
              " 'bhagat',\n",
              " 'is',\n",
              " 'a',\n",
              " 'symbol',\n",
              " 'of',\n",
              " 'new',\n",
              " 'india',\n",
              " 'a',\n",
              " 'torch',\n",
              " 'bearer',\n",
              " 'for',\n",
              " 'an',\n",
              " 'unafraid',\n",
              " 'generation1',\n",
              " 'chetan',\n",
              " 'bhagat',\n",
              " 'was',\n",
              " 'born',\n",
              " 'on',\n",
              " '22',\n",
              " 'april',\n",
              " '1974',\n",
              " 'in',\n",
              " 'delhi',\n",
              " 'the',\n",
              " 'capital',\n",
              " 'of',\n",
              " 'india',\n",
              " 'in',\n",
              " 'a',\n",
              " 'middle',\n",
              " 'class',\n",
              " 'punjabi',\n",
              " 'family',\n",
              " 'his',\n",
              " 'father',\n",
              " 'was',\n",
              " 'in',\n",
              " 'army',\n",
              " 'and',\n",
              " 'his',\n",
              " 'mother',\n",
              " 'was',\n",
              " 'an',\n",
              " 'employee',\n",
              " 'in',\n",
              " 'a',\n",
              " 'government',\n",
              " 'department',\n",
              " 'he',\n",
              " 'got',\n",
              " 'his',\n",
              " 'early',\n",
              " 'education',\n",
              " 'in',\n",
              " 'delhi',\n",
              " 'first',\n",
              " 'he',\n",
              " 'joined',\n",
              " 'an',\n",
              " 'army',\n",
              " 'public',\n",
              " 'school',\n",
              " 'at',\n",
              " 'dhaula',\n",
              " 'kuan',\n",
              " 'from',\n",
              " '1978',\n",
              " 'to',\n",
              " '1991',\n",
              " 'his',\n",
              " 'early',\n",
              " 'childhood',\n",
              " 'memories',\n",
              " 'reflect',\n",
              " 'his',\n",
              " 'middle',\n",
              " 'class',\n",
              " 'family',\n",
              " 'as',\n",
              " 'this',\n",
              " 'is',\n",
              " 'known',\n",
              " 'through',\n",
              " 'one',\n",
              " 'of',\n",
              " 'his',\n",
              " 'articles',\n",
              " 'written',\n",
              " 'by',\n",
              " 'him',\n",
              " 'in',\n",
              " 'his',\n",
              " 'prose',\n",
              " 'colletion',\n",
              " 'what',\n",
              " 'young',\n",
              " 'india',\n",
              " 'wants',\n",
              " 'throughout',\n",
              " 'my',\n",
              " 'childhood',\n",
              " 'i',\n",
              " 'remember',\n",
              " 'the',\n",
              " 'shortage',\n",
              " 'of',\n",
              " 'money',\n",
              " 'being',\n",
              " 'a',\n",
              " 'constant',\n",
              " 'theme',\n",
              " 'in',\n",
              " 'the',\n",
              " 'house',\n",
              " 'we',\n",
              " 'had',\n",
              " 'enough',\n",
              " 'to',\n",
              " 'run',\n",
              " 'the',\n",
              " 'kitchen',\n",
              " 'and',\n",
              " 'pay',\n",
              " 'for',\n",
              " 'utilities',\n",
              " 'but',\n",
              " 'little',\n",
              " 'to',\n",
              " 'build',\n",
              " 'assets',\n",
              " 'on',\n",
              " 'or',\n",
              " 'make',\n",
              " 'expenses',\n",
              " 'for',\n",
              " 'instance',\n",
              " 'we',\n",
              " 'couldnt',\n",
              " 'repair',\n",
              " 'a',\n",
              " 'broken',\n",
              " 'sofa',\n",
              " 'for',\n",
              " 'years',\n",
              " 'when',\n",
              " 'guest',\n",
              " 'came',\n",
              " 'to',\n",
              " 'our',\n",
              " 'house',\n",
              " 'we',\n",
              " 'find',\n",
              " 'it',\n",
              " 'expensive',\n",
              " 'to',\n",
              " 'serve',\n",
              " 'the',\n",
              " 'coke',\n",
              " 'and',\n",
              " 'served',\n",
              " 'lemonade',\n",
              " 'instead',\n",
              " 'we',\n",
              " 'rarely',\n",
              " 'ate',\n",
              " 'out',\n",
              " 'in',\n",
              " 'restaurants',\n",
              " 'and',\n",
              " 'when',\n",
              " 'we',\n",
              " 'did',\n",
              " 'we',\n",
              " 'did',\n",
              " 'so',\n",
              " 'with',\n",
              " 'caution',\n",
              " 'figuring',\n",
              " 'out',\n",
              " 'the',\n",
              " 'cheapest',\n",
              " 'and',\n",
              " 'the',\n",
              " 'most',\n",
              " 'filling',\n",
              " 'sic',\n",
              " 'items',\n",
              " 'on',\n",
              " 'the',\n",
              " 'menu2',\n",
              " 'it',\n",
              " 'was',\n",
              " 'these',\n",
              " 'experiences',\n",
              " 'that',\n",
              " 'made',\n",
              " 'him',\n",
              " 'to',\n",
              " 'work',\n",
              " 'hard',\n",
              " 'to',\n",
              " 'get',\n",
              " 'rid',\n",
              " 'of',\n",
              " 'his',\n",
              " 'financial',\n",
              " 'crises',\n",
              " 'bhagat',\n",
              " 'keen',\n",
              " 'interest',\n",
              " 'in',\n",
              " 'science',\n",
              " 'helped',\n",
              " 'him',\n",
              " 'to',\n",
              " 'go',\n",
              " 'for',\n",
              " 'iit',\n",
              " 'exams',\n",
              " 'he',\n",
              " 'cracked',\n",
              " 'the',\n",
              " 'iit',\n",
              " 'examination',\n",
              " 'by',\n",
              " 'securing',\n",
              " 'a',\n",
              " 'good',\n",
              " 'rank',\n",
              " 'to',\n",
              " 'get',\n",
              " 'into',\n",
              " 'indian',\n",
              " 'institute',\n",
              " 'of',\n",
              " 'technology',\n",
              " 'iit',\n",
              " 'delhi',\n",
              " 'for',\n",
              " 'mechanical',\n",
              " 'engineering',\n",
              " 'in',\n",
              " '1991',\n",
              " 'it',\n",
              " 'was',\n",
              " 'turning',\n",
              " 'point',\n",
              " 'in',\n",
              " 'his',\n",
              " 'life',\n",
              " 'as',\n",
              " 'is',\n",
              " 'remarked',\n",
              " 'by',\n",
              " 'him',\n",
              " 'iit',\n",
              " 'did',\n",
              " 'for',\n",
              " 'me',\n",
              " 'what',\n",
              " 'liberalisation',\n",
              " 'did',\n",
              " 'for',\n",
              " 'india',\n",
              " 'created',\n",
              " 'opportunities',\n",
              " 'and',\n",
              " 'changed',\n",
              " 'my',\n",
              " 'life',\n",
              " 'forever3',\n",
              " 'chetan',\n",
              " 'bhagat',\n",
              " 'was',\n",
              " 'humorous',\n",
              " 'from',\n",
              " 'his',\n",
              " 'childhood',\n",
              " 'he',\n",
              " 'used',\n",
              " 'to',\n",
              " 'entertain',\n",
              " 'people',\n",
              " 'in',\n",
              " 'parties',\n",
              " 'using',\n",
              " 'his',\n",
              " 'jokes',\n",
              " 'and',\n",
              " 'no',\n",
              " 'one',\n",
              " 'took',\n",
              " 'him',\n",
              " 'seriously',\n",
              " 'once',\n",
              " 'he',\n",
              " 'was',\n",
              " 'advised',\n",
              " 'by',\n",
              " 'one',\n",
              " 'of',\n",
              " 'his',\n",
              " 'relatives',\n",
              " 'to',\n",
              " 'take',\n",
              " 'life',\n",
              " 'and',\n",
              " 'career',\n",
              " 'seriously',\n",
              " 'it',\n",
              " 'is',\n",
              " 'humorous',\n",
              " 'feature',\n",
              " 'in',\n",
              " 'his',\n",
              " 'personality',\n",
              " 'that',\n",
              " 'made',\n",
              " 'him',\n",
              " 'a',\n",
              " 'great',\n",
              " 'entertainer',\n",
              " 'of',\n",
              " 'the',\n",
              " 'contemporary',\n",
              " 'times',\n",
              " 'bhagats',\n",
              " 'novels',\n",
              " 'are',\n",
              " 'filled',\n",
              " 'many',\n",
              " 'funny',\n",
              " 'characters',\n",
              " 'representing',\n",
              " 'his',\n",
              " 'funny',\n",
              " 'nature',\n",
              " 'bhagats',\n",
              " 'latest',\n",
              " 'novel',\n",
              " 'half',\n",
              " 'girlfriend',\n",
              " 'published',\n",
              " 'in',\n",
              " '2014',\n",
              " 'portrays',\n",
              " 'young',\n",
              " 'adult',\n",
              " 'romance',\n",
              " 'set',\n",
              " 'in',\n",
              " 'the',\n",
              " 'rural',\n",
              " 'bihar',\n",
              " 'delhi',\n",
              " 'patna',\n",
              " 'and',\n",
              " 'new',\n",
              " 'york',\n",
              " 'the',\n",
              " 'novel',\n",
              " 'represents',\n",
              " 'the',\n",
              " 'struggle',\n",
              " 'of',\n",
              " 'a',\n",
              " 'bihari',\n",
              " 'boy',\n",
              " 'madhav',\n",
              " 'who',\n",
              " 'joins',\n",
              " 'a',\n",
              " 'college',\n",
              " 'in',\n",
              " 'delhi',\n",
              " 'where',\n",
              " 'he',\n",
              " 'comes',\n",
              " 'in',\n",
              " 'contact',\n",
              " 'with',\n",
              " 'a',\n",
              " 'girl',\n",
              " 'riya',\n",
              " 'from',\n",
              " 'delhi',\n",
              " 'falls',\n",
              " 'in',\n",
              " 'love',\n",
              " 'with',\n",
              " 'her',\n",
              " 'the',\n",
              " 'novel',\n",
              " 'reflects',\n",
              " 'his',\n",
              " 'struggle',\n",
              " 'helplessness',\n",
              " 'and',\n",
              " 'success',\n",
              " 'to',\n",
              " 'get',\n",
              " 'the',\n",
              " 'goals',\n",
              " 'of',\n",
              " 'his',\n",
              " 'life',\n",
              " 'the',\n",
              " 'novel',\n",
              " 'starts',\n",
              " 'with',\n",
              " 'the',\n",
              " 'interaction',\n",
              " 'between',\n",
              " 'chetan',\n",
              " 'bhagat',\n",
              " 'and',\n",
              " 'madhav',\n",
              " 'jha',\n",
              " 'in',\n",
              " 'a',\n",
              " 'hotel',\n",
              " 'the',\n",
              " 'interaction',\n",
              " 'between',\n",
              " 'them',\n",
              " 'inspires',\n",
              " 'the',\n",
              " 'author',\n",
              " 'to',\n",
              " 'convert',\n",
              " 'the',\n",
              " 'story',\n",
              " 'into',\n",
              " 'a',\n",
              " 'novel',\n",
              " 'which',\n",
              " 'is',\n",
              " 'known',\n",
              " 'by',\n",
              " 'the',\n",
              " 'name',\n",
              " 'of',\n",
              " 'half',\n",
              " 'girlfriend',\n",
              " 'the',\n",
              " 'title',\n",
              " 'of',\n",
              " 'the',\n",
              " 'novel',\n",
              " 'suggests',\n",
              " 'that',\n",
              " 'most',\n",
              " 'of',\n",
              " 'the',\n",
              " 'story',\n",
              " 'of',\n",
              " 'the',\n",
              " 'novel',\n",
              " 'revolves',\n",
              " 'around',\n",
              " 'the',\n",
              " 'romantic',\n",
              " 'relationship',\n",
              " 'between',\n",
              " 'the',\n",
              " 'two',\n",
              " 'madhav',\n",
              " 'jha',\n",
              " 'tells',\n",
              " 'the',\n",
              " 'author',\n",
              " 'how',\n",
              " 'he',\n",
              " 'comes',\n",
              " 'to',\n",
              " 'delhi',\n",
              " 'where',\n",
              " 'he',\n",
              " 'gets',\n",
              " 'admission',\n",
              " 'at',\n",
              " 'saint',\n",
              " 'stephens',\n",
              " 'college',\n",
              " 'under',\n",
              " 'sports',\n",
              " 'quota',\n",
              " 'after',\n",
              " 'the',\n",
              " 'interview',\n",
              " 'he',\n",
              " 'goes',\n",
              " 'out',\n",
              " 'for',\n",
              " 'trial',\n",
              " 'in',\n",
              " 'basket',\n",
              " 'ball',\n",
              " 'lawn',\n",
              " 'and',\n",
              " 'has',\n",
              " 'his',\n",
              " 'look',\n",
              " 'on',\n",
              " 'pretty',\n",
              " 'girl',\n",
              " 'playing',\n",
              " 'in',\n",
              " 'the',\n",
              " 'lawn',\n",
              " 'being',\n",
              " 'himself',\n",
              " 'from',\n",
              " 'the',\n",
              " 'same',\n",
              " 'field',\n",
              " 'he',\n",
              " 'gives',\n",
              " 'some',\n",
              " 'suggestions',\n",
              " 'to',\n",
              " 'the',\n",
              " 'pretty',\n",
              " 'girl',\n",
              " 'through',\n",
              " 'this',\n",
              " 'way',\n",
              " 'he',\n",
              " 'comes',\n",
              " 'in',\n",
              " 'a',\n",
              " 'contact',\n",
              " 'with',\n",
              " 'riya',\n",
              " 'riya',\n",
              " 'was',\n",
              " 'from',\n",
              " 'delhi',\n",
              " 'whereas',\n",
              " 'madhav',\n",
              " 'jha',\n",
              " 'was',\n",
              " 'from',\n",
              " 'bihar',\n",
              " 'their',\n",
              " 'life',\n",
              " 'was',\n",
              " 'completely',\n",
              " 'different',\n",
              " 'as',\n",
              " 'one',\n",
              " 'was',\n",
              " 'from',\n",
              " 'rural',\n",
              " 'india',\n",
              " 'and',\n",
              " 'another',\n",
              " 'one',\n",
              " 'from',\n",
              " 'urban',\n",
              " 'india',\n",
              " 'their',\n",
              " 'friendship',\n",
              " 'was',\n",
              " 'strange',\n",
              " 'combination',\n",
              " 'as',\n",
              " 'their',\n",
              " 'life',\n",
              " '1',\n",
              " 'journal',\n",
              " 'of',\n",
              " 'literature',\n",
              " 'languages',\n",
              " 'and',\n",
              " 'linguistics',\n",
              " 'wwwiisteorg',\n",
              " 'issn',\n",
              " '24228435',\n",
              " 'an',\n",
              " 'international',\n",
              " 'peerreviewed',\n",
              " 'journal',\n",
              " 'vol28',\n",
              " '2016',\n",
              " 'background',\n",
              " 'and',\n",
              " 'thinking',\n",
              " 'was',\n",
              " 'completely',\n",
              " 'antithesis',\n",
              " 'of',\n",
              " 'each',\n",
              " 'other',\n",
              " 'in',\n",
              " 'this',\n",
              " 'way',\n",
              " 'their',\n",
              " 'friendship',\n",
              " 'starts',\n",
              " 'that',\n",
              " 'was',\n",
              " 'surprise',\n",
              " 'for',\n",
              " 'everyone',\n",
              " 'in',\n",
              " 'the',\n",
              " 'college',\n",
              " 'the',\n",
              " 'reason',\n",
              " 'for',\n",
              " 'that',\n",
              " 'was',\n",
              " 'that',\n",
              " 'a',\n",
              " 'beautiful',\n",
              " 'urban',\n",
              " 'indian',\n",
              " 'girl',\n",
              " 'with',\n",
              " 'a',\n",
              " 'bihari',\n",
              " 'boy',\n",
              " 'was',\n",
              " 'beyond',\n",
              " 'the',\n",
              " 'reason',\n",
              " 'of',\n",
              " 'the',\n",
              " 'people',\n",
              " 'the',\n",
              " 'novel',\n",
              " 'portrays',\n",
              " 'beautifully',\n",
              " 'how',\n",
              " 'they',\n",
              " 'spend',\n",
              " 'time',\n",
              " 'in',\n",
              " 'the',\n",
              " 'college',\n",
              " 'and',\n",
              " 'after',\n",
              " 'that',\n",
              " 'outside',\n",
              " 'the',\n",
              " 'college',\n",
              " 'in',\n",
              " 'the',\n",
              " 'beginning',\n",
              " 'it',\n",
              " 'is',\n",
              " 'seen',\n",
              " 'that',\n",
              " 'how',\n",
              " 'madhav',\n",
              " 'just',\n",
              " 'feel',\n",
              " 'helplessness',\n",
              " 'before',\n",
              " 'everyone',\n",
              " 'whether',\n",
              " 'in',\n",
              " 'the',\n",
              " 'interview',\n",
              " 'before',\n",
              " 'the',\n",
              " 'selection',\n",
              " 'committee',\n",
              " 'or',\n",
              " 'outside',\n",
              " 'facing',\n",
              " 'other',\n",
              " 'boys',\n",
              " 'in',\n",
              " 'the',\n",
              " 'college',\n",
              " 'as',\n",
              " 'madhav',\n",
              " 'was',\n",
              " 'from',\n",
              " 'rural',\n",
              " 'india',\n",
              " 'he',\n",
              " 'was',\n",
              " 'not',\n",
              " 'comfortable',\n",
              " 'with',\n",
              " 'english',\n",
              " 'at',\n",
              " 'the',\n",
              " 'time',\n",
              " 'of',\n",
              " 'interview',\n",
              " 'before',\n",
              " 'the',\n",
              " 'professors',\n",
              " 'it',\n",
              " 'shows',\n",
              " 'his',\n",
              " 'helplessness',\n",
              " 'with',\n",
              " 'english',\n",
              " 'as',\n",
              " 'is',\n",
              " 'known',\n",
              " 'from',\n",
              " 'his',\n",
              " 'own',\n",
              " 'words',\n",
              " 'i',\n",
              " 'am',\n",
              " 'from',\n",
              " 'rural',\n",
              " 'india',\n",
              " 'i',\n",
              " 'am',\n",
              " 'from',\n",
              " 'a',\n",
              " 'rural',\n",
              " 'area',\n",
              " 'gupta',\n",
              " 'said',\n",
              " 'emphasising',\n",
              " 'the',\n",
              " 'a',\n",
              " 'as',\n",
              " 'if',\n",
              " 'omitting',\n",
              " 'it',\n",
              " 'was',\n",
              " 'a',\n",
              " 'criminal',\n",
              " 'offence',\n",
              " 'hindi',\n",
              " 'sir',\n",
              " 'can',\n",
              " 'i',\n",
              " 'explain',\n",
              " 'in',\n",
              " 'hindibhagat',\n",
              " '20',\n",
              " 'at',\n",
              " 'another',\n",
              " 'place',\n",
              " 'when',\n",
              " 'he',\n",
              " 'meets',\n",
              " 'riya',\n",
              " 'he',\n",
              " 'could',\n",
              " 'not',\n",
              " 'feel',\n",
              " 'comfortable',\n",
              " 'to',\n",
              " 'speak',\n",
              " 'to',\n",
              " 'her',\n",
              " 'after',\n",
              " 'watching',\n",
              " 'riya',\n",
              " 'speaking',\n",
              " 'fluent',\n",
              " 'english',\n",
              " 'as',\n",
              " 'we',\n",
              " 'see',\n",
              " 'in',\n",
              " 'the',\n",
              " 'text',\n",
              " 'that',\n",
              " 'portrays',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1X-gZehIVnX"
      },
      "source": [
        "#Unique tokens\n",
        "Another useful data structure in Python is the set which is an unordered collection of distinct objects Arranging the tokens in a set means that they will put only once, and could be a smaller data collection useful to see how many distinct tokens are in a text or to see if a specific token is in the text or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SaPrdJY1Hr_Q",
        "outputId": "0ad7918a-e523-4322-bc69-230d89244a02"
      },
      "source": [
        "uniqueTokens = set(textTokens)\n",
        "print (\"The text has {} unique tokens\".format (len(uniqueTokens)))\n",
        "print (\" -> lexical diversity: each token in average is repeated {} times\".format(len(textTokens) / len(uniqueTokens)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The text has 959 unique tokens\n",
            " -> lexical diversity: each token in average is repeated 3.4535974973931176 times\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ViL1C4qvIv06",
        "outputId": "5f829c43-34ed-43dd-a24d-8fc2ca2acf5b"
      },
      "source": [
        "sorted(uniqueTokens)[200:205]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['constant', 'contact', 'contemporary', 'control', 'convert']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6AkOxPNbI86f"
      },
      "source": [
        "#Remove stopwords"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST9y37l1Iy8q",
        "outputId": "85e4b047-a5e1-426e-882e-27342009d43c"
      },
      "source": [
        "getMostCommonWords(textTokens, 5)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 139), ('to', 123), ('of', 87), ('in', 86), ('her', 79)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_mtGcg9JEaN"
      },
      "source": [
        "As you can see the most common words are not really meaningful but we can remove them\n",
        "\n",
        "Stopwords (https://en.wikipedia.org/wiki/Stop_words) are common (English) words that do not contribute much to the content or meaning of a document (e.g., \"the\", \"a\", \"is\", \"to\", etc.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmhKzfQLJAp2"
      },
      "source": [
        "'''f = open(\"stopwords.txt\")\n",
        "stopWordsText = f.read().splitlines()  # splitlines is used to remove newlines\n",
        "f.close()'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6-jDrzYKfWC"
      },
      "source": [
        "#stopWords = set(stopWordsText)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qmZ_mUCKmig",
        "outputId": "e997c8a1-90b5-4620-a622-41309c9ae0a1"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIog8b3hLCww"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_word = set(stopwords.words('english'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS0oXKnQLWJn"
      },
      "source": [
        "betterTokens = [token for token in textTokens if token not in stop_word]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e_0VYpJL3iX",
        "outputId": "f41233e7-d34b-4975-ffd7-58e531902fe4"
      },
      "source": [
        "betterTokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['elephants',\n",
              " '4',\n",
              " 'legs',\n",
              " 'pub',\n",
              " 'hi',\n",
              " 'super_friend',\n",
              " 'cant',\n",
              " 'believe',\n",
              " 'happened',\n",
              " 'common',\n",
              " 'friend',\n",
              " 'butcher',\n",
              " 'abstract',\n",
              " 'chetan',\n",
              " 'bhagat',\n",
              " 'novelist',\n",
              " 'whose',\n",
              " 'area',\n",
              " 'concern',\n",
              " 'length',\n",
              " 'breadth',\n",
              " 'entire',\n",
              " 'country',\n",
              " 'especially',\n",
              " 'urban',\n",
              " 'areas',\n",
              " 'uses',\n",
              " 'different',\n",
              " 'techniques',\n",
              " 'writings',\n",
              " 'write',\n",
              " 'india',\n",
              " 'writes',\n",
              " 'free',\n",
              " 'india',\n",
              " 'conservative',\n",
              " 'attitudes',\n",
              " 'people',\n",
              " 'false',\n",
              " 'myths',\n",
              " 'hopes',\n",
              " 'writings',\n",
              " 'dissolve',\n",
              " 'boundaries',\n",
              " 'people',\n",
              " 'different',\n",
              " 'backgrounds',\n",
              " 'cultures',\n",
              " 'communities',\n",
              " 'different',\n",
              " 'spaces',\n",
              " 'writers',\n",
              " 'country',\n",
              " 'tried',\n",
              " 'best',\n",
              " 'explore',\n",
              " 'struggles',\n",
              " 'challenges',\n",
              " 'new',\n",
              " 'generation',\n",
              " 'chetan',\n",
              " 'bhagat',\n",
              " 'chiefly',\n",
              " 'one',\n",
              " 'writers',\n",
              " 'whose',\n",
              " 'novels',\n",
              " 'representation',\n",
              " 'issues',\n",
              " 'novel',\n",
              " 'half',\n",
              " 'girlfriend',\n",
              " 'portrays',\n",
              " 'true',\n",
              " 'picture',\n",
              " 'indian',\n",
              " 'society',\n",
              " 'contemporary',\n",
              " 'times',\n",
              " 'paper',\n",
              " 'explores',\n",
              " 'struggles',\n",
              " 'characters',\n",
              " 'pass',\n",
              " 'hardships',\n",
              " 'get',\n",
              " 'success',\n",
              " 'keywords',\n",
              " 'multiculturalism',\n",
              " 'new',\n",
              " 'generation',\n",
              " 'inferiority',\n",
              " 'complex',\n",
              " 'individuality',\n",
              " 'cheatn',\n",
              " 'bhagat',\n",
              " 'novelist',\n",
              " 'expresses',\n",
              " 'profound',\n",
              " 'love',\n",
              " 'country',\n",
              " 'writing',\n",
              " 'portrays',\n",
              " 'issues',\n",
              " 'writing',\n",
              " 'close',\n",
              " 'modern',\n",
              " 'generation',\n",
              " 'comments',\n",
              " 'faulty',\n",
              " 'education',\n",
              " 'system',\n",
              " 'communal',\n",
              " 'racial',\n",
              " 'society',\n",
              " 'problems',\n",
              " 'indian',\n",
              " 'youth',\n",
              " 'therefore',\n",
              " 'necessary',\n",
              " 'indian',\n",
              " 'todays',\n",
              " 'comments',\n",
              " 'regarding',\n",
              " 'bhagat',\n",
              " 'symbol',\n",
              " 'new',\n",
              " 'india',\n",
              " 'torch',\n",
              " 'bearer',\n",
              " 'unafraid',\n",
              " 'generation1',\n",
              " 'chetan',\n",
              " 'bhagat',\n",
              " 'born',\n",
              " '22',\n",
              " 'april',\n",
              " '1974',\n",
              " 'delhi',\n",
              " 'capital',\n",
              " 'india',\n",
              " 'middle',\n",
              " 'class',\n",
              " 'punjabi',\n",
              " 'family',\n",
              " 'father',\n",
              " 'army',\n",
              " 'mother',\n",
              " 'employee',\n",
              " 'government',\n",
              " 'department',\n",
              " 'got',\n",
              " 'early',\n",
              " 'education',\n",
              " 'delhi',\n",
              " 'first',\n",
              " 'joined',\n",
              " 'army',\n",
              " 'public',\n",
              " 'school',\n",
              " 'dhaula',\n",
              " 'kuan',\n",
              " '1978',\n",
              " '1991',\n",
              " 'early',\n",
              " 'childhood',\n",
              " 'memories',\n",
              " 'reflect',\n",
              " 'middle',\n",
              " 'class',\n",
              " 'family',\n",
              " 'known',\n",
              " 'one',\n",
              " 'articles',\n",
              " 'written',\n",
              " 'prose',\n",
              " 'colletion',\n",
              " 'young',\n",
              " 'india',\n",
              " 'wants',\n",
              " 'throughout',\n",
              " 'childhood',\n",
              " 'remember',\n",
              " 'shortage',\n",
              " 'money',\n",
              " 'constant',\n",
              " 'theme',\n",
              " 'house',\n",
              " 'enough',\n",
              " 'run',\n",
              " 'kitchen',\n",
              " 'pay',\n",
              " 'utilities',\n",
              " 'little',\n",
              " 'build',\n",
              " 'assets',\n",
              " 'make',\n",
              " 'expenses',\n",
              " 'instance',\n",
              " 'couldnt',\n",
              " 'repair',\n",
              " 'broken',\n",
              " 'sofa',\n",
              " 'years',\n",
              " 'guest',\n",
              " 'came',\n",
              " 'house',\n",
              " 'find',\n",
              " 'expensive',\n",
              " 'serve',\n",
              " 'coke',\n",
              " 'served',\n",
              " 'lemonade',\n",
              " 'instead',\n",
              " 'rarely',\n",
              " 'ate',\n",
              " 'restaurants',\n",
              " 'caution',\n",
              " 'figuring',\n",
              " 'cheapest',\n",
              " 'filling',\n",
              " 'sic',\n",
              " 'items',\n",
              " 'menu2',\n",
              " 'experiences',\n",
              " 'made',\n",
              " 'work',\n",
              " 'hard',\n",
              " 'get',\n",
              " 'rid',\n",
              " 'financial',\n",
              " 'crises',\n",
              " 'bhagat',\n",
              " 'keen',\n",
              " 'interest',\n",
              " 'science',\n",
              " 'helped',\n",
              " 'go',\n",
              " 'iit',\n",
              " 'exams',\n",
              " 'cracked',\n",
              " 'iit',\n",
              " 'examination',\n",
              " 'securing',\n",
              " 'good',\n",
              " 'rank',\n",
              " 'get',\n",
              " 'indian',\n",
              " 'institute',\n",
              " 'technology',\n",
              " 'iit',\n",
              " 'delhi',\n",
              " 'mechanical',\n",
              " 'engineering',\n",
              " '1991',\n",
              " 'turning',\n",
              " 'point',\n",
              " 'life',\n",
              " 'remarked',\n",
              " 'iit',\n",
              " 'liberalisation',\n",
              " 'india',\n",
              " 'created',\n",
              " 'opportunities',\n",
              " 'changed',\n",
              " 'life',\n",
              " 'forever3',\n",
              " 'chetan',\n",
              " 'bhagat',\n",
              " 'humorous',\n",
              " 'childhood',\n",
              " 'used',\n",
              " 'entertain',\n",
              " 'people',\n",
              " 'parties',\n",
              " 'using',\n",
              " 'jokes',\n",
              " 'one',\n",
              " 'took',\n",
              " 'seriously',\n",
              " 'advised',\n",
              " 'one',\n",
              " 'relatives',\n",
              " 'take',\n",
              " 'life',\n",
              " 'career',\n",
              " 'seriously',\n",
              " 'humorous',\n",
              " 'feature',\n",
              " 'personality',\n",
              " 'made',\n",
              " 'great',\n",
              " 'entertainer',\n",
              " 'contemporary',\n",
              " 'times',\n",
              " 'bhagats',\n",
              " 'novels',\n",
              " 'filled',\n",
              " 'many',\n",
              " 'funny',\n",
              " 'characters',\n",
              " 'representing',\n",
              " 'funny',\n",
              " 'nature',\n",
              " 'bhagats',\n",
              " 'latest',\n",
              " 'novel',\n",
              " 'half',\n",
              " 'girlfriend',\n",
              " 'published',\n",
              " '2014',\n",
              " 'portrays',\n",
              " 'young',\n",
              " 'adult',\n",
              " 'romance',\n",
              " 'set',\n",
              " 'rural',\n",
              " 'bihar',\n",
              " 'delhi',\n",
              " 'patna',\n",
              " 'new',\n",
              " 'york',\n",
              " 'novel',\n",
              " 'represents',\n",
              " 'struggle',\n",
              " 'bihari',\n",
              " 'boy',\n",
              " 'madhav',\n",
              " 'joins',\n",
              " 'college',\n",
              " 'delhi',\n",
              " 'comes',\n",
              " 'contact',\n",
              " 'girl',\n",
              " 'riya',\n",
              " 'delhi',\n",
              " 'falls',\n",
              " 'love',\n",
              " 'novel',\n",
              " 'reflects',\n",
              " 'struggle',\n",
              " 'helplessness',\n",
              " 'success',\n",
              " 'get',\n",
              " 'goals',\n",
              " 'life',\n",
              " 'novel',\n",
              " 'starts',\n",
              " 'interaction',\n",
              " 'chetan',\n",
              " 'bhagat',\n",
              " 'madhav',\n",
              " 'jha',\n",
              " 'hotel',\n",
              " 'interaction',\n",
              " 'inspires',\n",
              " 'author',\n",
              " 'convert',\n",
              " 'story',\n",
              " 'novel',\n",
              " 'known',\n",
              " 'name',\n",
              " 'half',\n",
              " 'girlfriend',\n",
              " 'title',\n",
              " 'novel',\n",
              " 'suggests',\n",
              " 'story',\n",
              " 'novel',\n",
              " 'revolves',\n",
              " 'around',\n",
              " 'romantic',\n",
              " 'relationship',\n",
              " 'two',\n",
              " 'madhav',\n",
              " 'jha',\n",
              " 'tells',\n",
              " 'author',\n",
              " 'comes',\n",
              " 'delhi',\n",
              " 'gets',\n",
              " 'admission',\n",
              " 'saint',\n",
              " 'stephens',\n",
              " 'college',\n",
              " 'sports',\n",
              " 'quota',\n",
              " 'interview',\n",
              " 'goes',\n",
              " 'trial',\n",
              " 'basket',\n",
              " 'ball',\n",
              " 'lawn',\n",
              " 'look',\n",
              " 'pretty',\n",
              " 'girl',\n",
              " 'playing',\n",
              " 'lawn',\n",
              " 'field',\n",
              " 'gives',\n",
              " 'suggestions',\n",
              " 'pretty',\n",
              " 'girl',\n",
              " 'way',\n",
              " 'comes',\n",
              " 'contact',\n",
              " 'riya',\n",
              " 'riya',\n",
              " 'delhi',\n",
              " 'whereas',\n",
              " 'madhav',\n",
              " 'jha',\n",
              " 'bihar',\n",
              " 'life',\n",
              " 'completely',\n",
              " 'different',\n",
              " 'one',\n",
              " 'rural',\n",
              " 'india',\n",
              " 'another',\n",
              " 'one',\n",
              " 'urban',\n",
              " 'india',\n",
              " 'friendship',\n",
              " 'strange',\n",
              " 'combination',\n",
              " 'life',\n",
              " '1',\n",
              " 'journal',\n",
              " 'literature',\n",
              " 'languages',\n",
              " 'linguistics',\n",
              " 'wwwiisteorg',\n",
              " 'issn',\n",
              " '24228435',\n",
              " 'international',\n",
              " 'peerreviewed',\n",
              " 'journal',\n",
              " 'vol28',\n",
              " '2016',\n",
              " 'background',\n",
              " 'thinking',\n",
              " 'completely',\n",
              " 'antithesis',\n",
              " 'way',\n",
              " 'friendship',\n",
              " 'starts',\n",
              " 'surprise',\n",
              " 'everyone',\n",
              " 'college',\n",
              " 'reason',\n",
              " 'beautiful',\n",
              " 'urban',\n",
              " 'indian',\n",
              " 'girl',\n",
              " 'bihari',\n",
              " 'boy',\n",
              " 'beyond',\n",
              " 'reason',\n",
              " 'people',\n",
              " 'novel',\n",
              " 'portrays',\n",
              " 'beautifully',\n",
              " 'spend',\n",
              " 'time',\n",
              " 'college',\n",
              " 'outside',\n",
              " 'college',\n",
              " 'beginning',\n",
              " 'seen',\n",
              " 'madhav',\n",
              " 'feel',\n",
              " 'helplessness',\n",
              " 'everyone',\n",
              " 'whether',\n",
              " 'interview',\n",
              " 'selection',\n",
              " 'committee',\n",
              " 'outside',\n",
              " 'facing',\n",
              " 'boys',\n",
              " 'college',\n",
              " 'madhav',\n",
              " 'rural',\n",
              " 'india',\n",
              " 'comfortable',\n",
              " 'english',\n",
              " 'time',\n",
              " 'interview',\n",
              " 'professors',\n",
              " 'shows',\n",
              " 'helplessness',\n",
              " 'english',\n",
              " 'known',\n",
              " 'words',\n",
              " 'rural',\n",
              " 'india',\n",
              " 'rural',\n",
              " 'area',\n",
              " 'gupta',\n",
              " 'said',\n",
              " 'emphasising',\n",
              " 'omitting',\n",
              " 'criminal',\n",
              " 'offence',\n",
              " 'hindi',\n",
              " 'sir',\n",
              " 'explain',\n",
              " 'hindibhagat',\n",
              " '20',\n",
              " 'another',\n",
              " 'place',\n",
              " 'meets',\n",
              " 'riya',\n",
              " 'could',\n",
              " 'feel',\n",
              " 'comfortable',\n",
              " 'speak',\n",
              " 'watching',\n",
              " 'riya',\n",
              " 'speaking',\n",
              " 'fluent',\n",
              " 'english',\n",
              " 'see',\n",
              " 'text',\n",
              " 'portrays',\n",
              " 'beautifully',\n",
              " 'helplessness',\n",
              " 'madhav',\n",
              " 'english',\n",
              " 'language',\n",
              " 'english',\n",
              " 'said',\n",
              " 'see',\n",
              " 'thats',\n",
              " 'name',\n",
              " 'riya',\n",
              " 'somani',\n",
              " 'englishhons',\n",
              " 'said',\n",
              " 'heart',\n",
              " 'sank',\n",
              " 'girl',\n",
              " 'english',\n",
              " 'degree',\n",
              " 'would',\n",
              " 'never',\n",
              " 'befriend',\n",
              " 'country',\n",
              " 'bumpkin',\n",
              " 'like',\n",
              " 'mebhagat',\n",
              " '22',\n",
              " 'madhav',\n",
              " 'meeting',\n",
              " 'riya',\n",
              " 'develops',\n",
              " 'friendship',\n",
              " 'behaviour',\n",
              " 'towards',\n",
              " 'common',\n",
              " 'thought',\n",
              " 'indian',\n",
              " 'culture',\n",
              " 'believes',\n",
              " 'friendship',\n",
              " 'boyfriend',\n",
              " 'girlfriend',\n",
              " 'type',\n",
              " 'riya',\n",
              " 'modern',\n",
              " 'outlook',\n",
              " 'towards',\n",
              " 'life',\n",
              " 'agree',\n",
              " 'attitude',\n",
              " 'towards',\n",
              " 'often',\n",
              " 'admonishes',\n",
              " 'casual',\n",
              " 'like',\n",
              " 'common',\n",
              " 'friends',\n",
              " 'go',\n",
              " 'date',\n",
              " 'date',\n",
              " 'dual',\n",
              " 'meaning',\n",
              " 'madhav',\n",
              " 'takes',\n",
              " 'love',\n",
              " 'accepted',\n",
              " 'look',\n",
              " 'like',\n",
              " 'get',\n",
              " 'date',\n",
              " 'said',\n",
              " 'date',\n",
              " 'go',\n",
              " 'movie',\n",
              " 'like',\n",
              " 'friends',\n",
              " 'isnt',\n",
              " 'high',\n",
              " 'class',\n",
              " 'people',\n",
              " 'call',\n",
              " 'date',\n",
              " 'whats',\n",
              " 'date',\n",
              " 'want',\n",
              " 'see',\n",
              " 'movie',\n",
              " 'said',\n",
              " 'hands',\n",
              " 'hips',\n",
              " 'hands',\n",
              " 'onhips',\n",
              " 'pose',\n",
              " 'meant',\n",
              " 'question',\n",
              " 'three',\n",
              " 'months',\n",
              " 'known',\n",
              " 'knew',\n",
              " 'hated',\n",
              " 'pushed',\n",
              " 'thought',\n",
              " 'maybe',\n",
              " 'rich',\n",
              " 'people',\n",
              " 'somewhat',\n",
              " 'private',\n",
              " 'overdid',\n",
              " 'familiarity',\n",
              " 'villages',\n",
              " 'anyway',\n",
              " 'bhagat',\n",
              " '2930',\n",
              " 'evident',\n",
              " 'helpless',\n",
              " 'madhav',\n",
              " 'riya',\n",
              " 'tries',\n",
              " 'convince',\n",
              " 'accept',\n",
              " 'lover',\n",
              " 'way',\n",
              " 'wishes',\n",
              " 'riya',\n",
              " 'chides',\n",
              " 'limit',\n",
              " 'stops',\n",
              " 'cross',\n",
              " 'boundaries',\n",
              " 'beyond',\n",
              " 'friendship',\n",
              " 'madhav',\n",
              " 'perplexed',\n",
              " 'behaviour',\n",
              " 'doesnt',\n",
              " 'know',\n",
              " 'take',\n",
              " 'relationship',\n",
              " 'realises',\n",
              " 'time',\n",
              " 'different',\n",
              " 'places',\n",
              " 'people',\n",
              " 'think',\n",
              " 'number',\n",
              " 'questions',\n",
              " 'mind',\n",
              " 'seeks',\n",
              " 'answer',\n",
              " 'special',\n",
              " 'kept',\n",
              " 'asking',\n",
              " 'sometimes',\n",
              " 'saw',\n",
              " 'chatting',\n",
              " 'guys',\n",
              " 'felt',\n",
              " 'insanely',\n",
              " 'jealous',\n",
              " 'insistence',\n",
              " 'seeing',\n",
              " 'movie',\n",
              " 'together',\n",
              " 'find',\n",
              " 'riya',\n",
              " 'somani',\n",
              " 'really',\n",
              " 'thought',\n",
              " 'madhav',\n",
              " 'jha',\n",
              " 'held',\n",
              " 'hand',\n",
              " 'figure',\n",
              " 'stood',\n",
              " 'given',\n",
              " 'reactionnowhere',\n",
              " 'bhagat',\n",
              " '30',\n",
              " 'seeing',\n",
              " 'activities',\n",
              " 'madhav',\n",
              " 'riya',\n",
              " 'doesnt',\n",
              " 'allow',\n",
              " 'behave',\n",
              " 'like',\n",
              " 'tells',\n",
              " 'behave',\n",
              " 'proper',\n",
              " 'manner',\n",
              " 'although',\n",
              " 'madhav',\n",
              " 'tries',\n",
              " 'seduce',\n",
              " 'cinema',\n",
              " 'yet',\n",
              " 'doesnt',\n",
              " 'feel',\n",
              " 'angry',\n",
              " 'behaviour',\n",
              " 'takes',\n",
              " 'things',\n",
              " 'lightly',\n",
              " 'intention',\n",
              " 'towards',\n",
              " 'feelings',\n",
              " 'activities',\n",
              " 'fact',\n",
              " 'removed',\n",
              " 'arm',\n",
              " 'armrest',\n",
              " 'rest',\n",
              " 'movie',\n",
              " 'seems',\n",
              " 'upset',\n",
              " 'even',\n",
              " 'though',\n",
              " 'never',\n",
              " 'said',\n",
              " 'word',\n",
              " 'keeps',\n",
              " 'watching',\n",
              " 'film',\n",
              " 'everything',\n",
              " 'okey',\n",
              " 'said',\n",
              " 'sipped',\n",
              " 'drink',\n",
              " 'silence',\n",
              " 'walked',\n",
              " 'odeon',\n",
              " 'keventers',\n",
              " 'famous',\n",
              " 'milkshakes',\n",
              " 'sold',\n",
              " 'glasss',\n",
              " 'bottles',\n",
              " 'uh',\n",
              " 'huh',\n",
              " 'said',\n",
              " 'indicating',\n",
              " 'yes',\n",
              " 'hated',\n",
              " 'response',\n",
              " 'bhagat',\n",
              " '30',\n",
              " 'novel',\n",
              " 'also',\n",
              " 'portrays',\n",
              " 'helplessness',\n",
              " 'riya',\n",
              " 'family',\n",
              " 'wants',\n",
              " 'free',\n",
              " 'life',\n",
              " 'one',\n",
              " 'come',\n",
              " 'way',\n",
              " 'stop',\n",
              " 'fulfilling',\n",
              " 'dreams',\n",
              " 'doesnt',\n",
              " 'prove',\n",
              " 'successful',\n",
              " 'life',\n",
              " 'faces',\n",
              " 'lot',\n",
              " 'hurdles',\n",
              " 'family',\n",
              " 'makes',\n",
              " 'feel',\n",
              " 'sad',\n",
              " 'life',\n",
              " 'besides',\n",
              " 'study',\n",
              " 'wants',\n",
              " 'take',\n",
              " 'classes',\n",
              " 'music',\n",
              " 'singing',\n",
              " 'dream',\n",
              " 'life',\n",
              " 'discloses',\n",
              " 'first',\n",
              " 'time',\n",
              " 'madhav',\n",
              " 'family',\n",
              " 'acts',\n",
              " 'hurdle',\n",
              " 'fulfilment',\n",
              " 'dreams',\n",
              " 'dont',\n",
              " 'open',\n",
              " 'people',\n",
              " 'keep',\n",
              " 'journaland',\n",
              " 'even',\n",
              " 'rare',\n",
              " 'know',\n",
              " 'quiet',\n",
              " 'person',\n",
              " 'riya',\n",
              " 'said',\n",
              " 'understand',\n",
              " 'thanks',\n",
              " 'problem',\n",
              " 'family',\n",
              " 'obsessed',\n",
              " 'money',\n",
              " 'im',\n",
              " '2',\n",
              " 'journal',\n",
              " 'literature',\n",
              " 'languages',\n",
              " 'linguistics',\n",
              " 'wwwiisteorg',\n",
              " 'issn',\n",
              " '24228435',\n",
              " 'international',\n",
              " 'peerreviewed',\n",
              " 'journal',\n",
              " 'vol28',\n",
              " '2016',\n",
              " 'dont',\n",
              " 'know',\n",
              " 'also',\n",
              " 'dont',\n",
              " 'matter',\n",
              " 'brothers',\n",
              " 'take',\n",
              " 'business',\n",
              " 'one',\n",
              " 'day',\n",
              " 'im',\n",
              " 'supposed',\n",
              " 'shut',\n",
              " 'get',\n",
              " 'married',\n",
              " 'leave',\n",
              " 'high',\n",
              " 'point',\n",
              " 'life',\n",
              " 'kids',\n",
              " 'shop',\n",
              " 'want',\n",
              " 'almost',\n",
              " 'shouted',\n",
              " 'know',\n",
              " 'better',\n",
              " 'dont',\n",
              " 'sorry',\n",
              " 'sucks',\n",
              " 'girl',\n",
              " 'country',\n",
              " 'tell',\n",
              " 'sucks',\n",
              " 'seem',\n",
              " 'upset',\n",
              " 'something',\n",
              " 'happen',\n",
              " 'today',\n",
              " 'told',\n",
              " 'want',\n",
              " 'study',\n",
              " 'music',\n",
              " 'college',\n",
              " 'want',\n",
              " 'marry',\n",
              " 'rich',\n",
              " 'marwari',\n",
              " 'family',\n",
              " 'live',\n",
              " 'like',\n",
              " 'queen',\n",
              " 'dont',\n",
              " 'want',\n",
              " 'live',\n",
              " 'like',\n",
              " 'queen',\n",
              " 'dream',\n",
              " 'bhagat',\n",
              " '32',\n",
              " 'madhav',\n",
              " 'tries',\n",
              " 'encourage',\n",
              " 'feel',\n",
              " 'dreams',\n",
              " 'simple',\n",
              " 'attainable',\n",
              " 'tells',\n",
              " 'unreal',\n",
              " 'type',\n",
              " 'dreams',\n",
              " 'hard',\n",
              " 'attain',\n",
              " 'riya',\n",
              " 'believes',\n",
              " 'even',\n",
              " 'simple',\n",
              " 'dreams',\n",
              " 'family',\n",
              " 'cannot',\n",
              " 'fullfiled',\n",
              " 'smiled',\n",
              " 'nothing',\n",
              " 'simple',\n",
              " 'girl',\n",
              " 'family',\n",
              " 'like',\n",
              " 'mine',\n",
              " 'said',\n",
              " 'bhagat',\n",
              " '34',\n",
              " 'madahv',\n",
              " 'takes',\n",
              " 'relationship',\n",
              " 'riya',\n",
              " 'seriously',\n",
              " 'tries',\n",
              " 'hard',\n",
              " 'convince',\n",
              " 'riya',\n",
              " 'accept',\n",
              " 'love',\n",
              " 'girlfriend',\n",
              " 'goes',\n",
              " 'nowhere',\n",
              " 'riya',\n",
              " 'mood',\n",
              " 'make',\n",
              " 'boyfriend',\n",
              " 'riya',\n",
              " 'looks',\n",
              " 'different',\n",
              " 'type',\n",
              " 'girl',\n",
              " 'novel',\n",
              " 'looks',\n",
              " 'things',\n",
              " 'casual',\n",
              " 'manner',\n",
              " 'wants',\n",
              " 'friendship',\n",
              " 'madhav',\n",
              " 'want',\n",
              " 'move',\n",
              " 'ahead',\n",
              " 'beyond',\n",
              " 'one',\n",
              " 'point',\n",
              " 'time',\n",
              " 'classroom',\n",
              " 'thinking',\n",
              " 'comes',\n",
              " 'fore',\n",
              " 'yes',\n",
              " 'mr',\n",
              " 'jha',\n",
              " 'love',\n",
              " 'riya',\n",
              " 'burst',\n",
              " 'laughing',\n",
              " 'didnt',\n",
              " 'like',\n",
              " 'laughing',\n",
              " 'time',\n",
              " 'please',\n",
              " 'serious',\n",
              " 'casual',\n",
              " 'behaviour',\n",
              " 'hurts',\n",
              " 'said',\n",
              " 'composed',\n",
              " 'sat',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aC62ijQMAjT",
        "outputId": "f0209881-7899-400f-baff-ad48823f6bbc"
      },
      "source": [
        "getMostCommonWords(betterTokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('riya', 45),\n",
              " ('madhav', 33),\n",
              " ('bhagat', 23),\n",
              " ('said', 21),\n",
              " ('novel', 19),\n",
              " ('life', 18),\n",
              " ('love', 14),\n",
              " ('india', 12),\n",
              " ('one', 12),\n",
              " ('like', 12)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Qbm8m0QMf2f"
      },
      "source": [
        "Generate a words cloud from a text\n",
        "Small example: we generate a words cloud from a text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QO3LQ6YMXgP"
      },
      "source": [
        "from urllib.request import urlopen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R56lgcG6Miu5"
      },
      "source": [
        "def getWordCloud(filename):\n",
        "    textTokens = [] # tokens will be added here\n",
        "    lines = 0\n",
        "    path = \"http://www.gutenberg.org/files/\"\n",
        "\n",
        "    url = path + filename + \"/\" + filename + \"-0.txt\"\n",
        "    f = urlopen(url)\n",
        "    \n",
        "    for line in f:            \n",
        "               # every line gets processed\n",
        "        lineTokens = preprocessText(line.decode('utf-8'))               \n",
        "                # append the tokens to my list\n",
        "        textTokens.extend(lineTokens)\n",
        "        \n",
        "        lines += 1  # finally move to the next line\n",
        "            \n",
        "    '''fs = open(\"stopwords.txt\")\n",
        "    stopWordsText = fs.read().splitlines()\n",
        "    stopWords = set(stopWordsText)\n",
        "    fs.close()'''\n",
        "    betterTokens = [token for token in textTokens if token not in stop_Word]\n",
        "    \n",
        "    wordsCount = Counter(betterTokens) # count the occurences\n",
        "    \n",
        "          # put each token and its occurrence in a file\n",
        "    with open(\"wordcloud_\"+filename+\".txt\", 'a') as fw:\n",
        "        for line in wordsCount.most_common():\n",
        "    #        freq = (line[1] + 10 // 2) // 10\n",
        "     #       if freq > 2:\n",
        "            fw.write(str(line[1]) + ' ' + line[0] + '\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        },
        "id": "BuJ7UkxuMn31",
        "outputId": "07192930-1871-44f1-bb78-987a644a5ca2"
      },
      "source": [
        "getWordCloud(\"1342\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-e266fa1ebbdd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgetWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"1342\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-63-a49c4ba1ece2>\u001b[0m in \u001b[0;36mgetWordCloud\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mstopWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopWordsText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     fs.close()'''\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mbetterTokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextTokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_Word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mwordsCount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetterTokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# count the occurences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-63-a49c4ba1ece2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mstopWords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstopWordsText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     fs.close()'''\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mbetterTokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtextTokens\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_Word\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mwordsCount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbetterTokens\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# count the occurences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'stop_Word' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8_G9ogaMuGh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}